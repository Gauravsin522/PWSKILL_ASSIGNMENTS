{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Can we use Bagging for regression problems?\n",
    "\n",
    "# Answer:- Yes, we can use Bagging for regression problems. Bagging is a general-purpose procedure for reducing the variance of a statistical \n",
    "# learning method. When we use bagging with a regression tree, we generate B different bootstrapped training data sets. We then train a regression \n",
    "# tree on the bth bootstrapped training set in order to get ˆf b(x), and average all the predictions, ˆf b(x), to obtain the bagged estimate:\n",
    "\n",
    "# ˆf bag(x) = 1/B ∑ˆf b(x)\n",
    "# where ˆf bag(x) is the bagged estimate of f(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.What is the difference between multiple model training and single model training?\n",
    "\n",
    "# Answer:- In single model training, we train a single model on the entire dataset. In multiple model training, we train multiple models on\n",
    "# different subsets of the dataset. The predictions of the multiple models are then combined to make the final prediction. Multiple model training\n",
    "# can help reduce overfitting and improve the generalization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Explain the concept of feature randomness in Random Forest.\n",
    "\n",
    "# Answer:- Random Forest is an ensemble learning method that combines multiple decision trees to create a more accurate and robust model. One of the key\n",
    "# features of Random Forest is the use of feature randomness. Feature randomness refers to the random selection of a subset of features at each split\n",
    "# in the decision tree. This helps to reduce the correlation between the trees in the forest and improve the generalization of the model. By using\n",
    "# feature randomness, Random Forest is able to capture a wider range of patterns in the data and make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. What is OOB (Out-of-Bag) Score?\n",
    "\n",
    "# Answer:- The Out-of-Bag (OOB) score is a method for estimating the performance of a Random Forest model without the need for a separate validation set.\n",
    "# In Random Forest, each tree is trained on a bootstrapped sample of the data, and the remaining data points that are not included in the bootstrap sample\n",
    "# are called the out-of-bag samples. The OOB score is calculated by making predictions on the out-of-bag samples using the corresponding tree and then\n",
    "# aggregating the predictions across all trees in the forest. The OOB score provides an unbiased estimate of the model's performance on unseen data and\n",
    "# can be used to tune hyperparameters and evaluate the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How can you measure the importance of features in a Random Forest model?\n",
    "\n",
    "# Answer:- The importance of features in a Random Forest model can be measured using the feature_importances_ attribute of the RandomForestRegressor or\n",
    "# RandomForestClassifier class in scikit-learn. The feature_importances_ attribute returns an array of importance scores for each feature in the dataset.\n",
    "# The higher the importance score, the more important the feature is in predicting the target variable. Feature importance scores can be used to identify\n",
    "# the most relevant features in the dataset and to gain insights into the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Explain the working principle of a Bagging Classifier.\n",
    "\n",
    "# Answer:- Bagging (Bootstrap Aggregating) is an ensemble learning method that combines multiple base classifiers to create a more accurate and robust\n",
    "# model. The working principle of a Bagging Classifier is as follows:\n",
    "\n",
    "# 1. Generate B bootstrap samples from the training data set.\n",
    "\n",
    "# 2. Train a base classifier on each bootstrap sample to get B different classifiers.\n",
    "\n",
    "# 3. Combine the predictions of the B classifiers using a majority voting (for classification) or averaging (for regression) to make the final prediction.\n",
    "\n",
    "# By combining the predictions of multiple classifiers, Bagging reduces the variance of the model and improves the generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. How do you evaluate a Bagging Classifier’s performance?\n",
    "\n",
    "# Answer:- The performance of a Bagging Classifier can be evaluated using various metrics such as accuracy, precision, recall, F1 score, ROC-AUC, etc.\n",
    "# The choice of evaluation metric depends on the nature of the problem (classification or regression) and the specific requirements of the application.\n",
    "# Some common evaluation metrics for classification problems are:\n",
    "\n",
    "# 1. Accuracy: The proportion of correctly classified instances out of the total instances.\n",
    "\n",
    "# 2. Precision: The proportion of true positive predictions out of all positive predictions.\n",
    "\n",
    "# 3. Recall: The proportion of true positive predictions out of all actual positive instances.\n",
    "\n",
    "# 4. F1 score: The harmonic mean of precision and recall, which provides a balance between the two metrics.\n",
    "\n",
    "# 5. ROC-AUC: The area under the receiver operating characteristic curve, which measures the trade-off between true positive rate and false positive rate.\n",
    "\n",
    "# By evaluating the Bagging Classifier using these metrics, we can assess its performance and make informed decisions about model selection and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. How does a Bagging Regressor work?\n",
    "\n",
    "# Answer:- A Bagging Regressor is an ensemble learning method that combines multiple regression models to create a more accurate and robust model.\n",
    "# The working principle of a Bagging Regressor is similar to that of a Bagging Classifier, with the following key steps:\n",
    "\n",
    "# 1. Generate B bootstrap samples from the training data set.\n",
    "\n",
    "# 2. Train a base regression model on each bootstrap sample to get B different regression models.\n",
    "\n",
    "# 3. Combine the predictions of the B regression models by averaging the predictions to make the final prediction.\n",
    "\n",
    "# By combining the predictions of multiple regression models, Bagging Regressor reduces the variance of the model and improves the generalization ability.\n",
    "# The Bagging Regressor can be used to predict continuous target variables and is particularly effective when the base regression models are diverse and\n",
    "# capture different patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. What is the main advantage of ensemble techniques?\n",
    "\n",
    "# Answer:- The main advantage of ensemble techniques is that they can improve the predictive performance of machine learning models by combining the\n",
    "# predictions of multiple base models. Ensemble techniques leverage the diversity of the base models to reduce overfitting, increase generalization\n",
    "# ability, and make more accurate predictions. By combining the predictions of multiple models, ensemble techniques can capture a wider range of patterns\n",
    "# in the data and provide more robust and reliable predictions. Ensemble techniques are widely used in practice and have been shown to outperform\n",
    "# individual models in many machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. What is the main challenge of ensemble methods?\n",
    "\n",
    "# Answer:- The main challenge of ensemble methods is the increased complexity and computational cost associated with training and combining multiple models.\n",
    "# Ensemble methods require training and maintaining multiple base models, which can be computationally expensive and time-consuming, especially for large\n",
    "# datasets and complex models. In addition, ensemble methods may require tuning hyperparameters and selecting the appropriate combination of base models\n",
    "# to achieve optimal performance. The complexity of ensemble methods can make them difficult to interpret and debug, and may require additional computational\n",
    "# resources and expertise to implement effectively. Despite these challenges, ensemble methods are widely used in practice due to their ability to improve\n",
    "# predictive performance and generalization ability in machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Explain the key idea behind ensemble techniques.\n",
    "\n",
    "# Answer:- The key idea behind ensemble techniques is to combine the predictions of multiple base models to create a more accurate and robust model.\n",
    "# Ensemble techniques leverage the diversity of the base models to reduce overfitting, increase generalization ability, and make more accurate predictions.\n",
    "# By combining the predictions of multiple models, ensemble techniques can capture a wider range of patterns in the data and provide more reliable and\n",
    "# robust predictions. Ensemble techniques are based on the principle of \"wisdom of the crowd,\" where the collective knowledge of multiple models is\n",
    "# superior to that of any individual model. Ensemble techniques have been shown to outperform individual models in many machine learning tasks and are\n",
    "# widely used in practice to improve predictive performance and generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. What is a Random Forest Classifier?\n",
    "\n",
    "# Answer:- A Random Forest Classifier is an ensemble learning method that combines multiple decision trees to create a more accurate and robust\n",
    "# classification model. Random Forest is based on the bagging (bootstrap aggregating) technique, where multiple decision trees are trained on\n",
    "# bootstrapped samples of the training data and the predictions are combined using a majority voting scheme. Random Forest introduces additional\n",
    "# randomness by selecting a random subset of features at each split in the decision tree, which helps to reduce the correlation between the trees and\n",
    "# improve the generalization ability of the model. Random Forest is a powerful and versatile classifier that can handle high-dimensional data, non-linear\n",
    "# relationships, and noisy data, making it a popular choice for a wide range of classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.  What are the main types of ensemble techniques?\n",
    "\n",
    "# Answer:- The main types of ensemble techniques are:\n",
    "\n",
    "# 1. Bagging: Bagging (Bootstrap Aggregating) is an ensemble learning method that combines multiple base models trained on different subsets of the\n",
    "# training data to create a more accurate and robust model.\n",
    "\n",
    "# 2. Boosting: Boosting is an ensemble learning method that trains multiple base models sequentially, where each subsequent model focuses on the\n",
    "# instances that were misclassified by the previous models. Boosting aims to reduce bias and improve the generalization ability of the model.\n",
    "\n",
    "# 3. Random Forest: Random Forest is an ensemble learning method that combines multiple decision trees to create a more accurate and robust model.\n",
    "# Random Forest introduces additional randomness by selecting a random subset of features at each split in the decision tree to improve the generalization\n",
    "# ability of the model.\n",
    "\n",
    "# 4. Stacking: Stacking is an ensemble learning method that combines the predictions of multiple base models using a meta-model (or blender) to make the\n",
    "# final prediction. Stacking leverages the strengths of different base models to create a more accurate and robust model.\n",
    "\n",
    "# 5. Voting: Voting is an ensemble learning method that combines the predictions of multiple base models using a majority voting (for classification) or\n",
    "# averaging (for regression) to make the final prediction. Voting leverages the collective knowledge of multiple models to improve the predictive performance\n",
    "# of the model.\n",
    "\n",
    "# These ensemble techniques can be used in combination or individually to improve the predictive performance and generalization ability of machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. What is ensemble learning in machine learning?\n",
    "\n",
    "# Answer:- Ensemble learning is a machine learning technique that combines the predictions of multiple base models to create a more accurate and robust\n",
    "# model. Ensemble learning leverages the diversity of the base models to reduce overfitting, increase generalization ability, and make more accurate\n",
    "# predictions. By combining the predictions of multiple models, ensemble learning can capture a wider range of patterns in the data and provide more\n",
    "# reliable and robust predictions. Ensemble learning is based on the principle of \"wisdom of the crowd,\" where the collective knowledge of multiple models\n",
    "# is superior to that of any individual model. Ensemble learning has been shown to outperform individual models in many machine learning tasks and is\n",
    "# widely used in practice to improve predictive performance and generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. When should we avoid using ensemble methods?\n",
    "\n",
    "# Answer:- Ensemble methods should be avoided in the following cases:\n",
    "\n",
    "# 1. When the dataset is small: Ensemble methods may not be suitable for small datasets as they require training multiple base models, which can lead to\n",
    "# overfitting and high variance.\n",
    "\n",
    "# 2. When the base models are highly correlated: If the base models are highly correlated, ensemble methods may not provide significant improvements in\n",
    "# predictive performance, as the models are likely to make similar predictions.\n",
    "\n",
    "# 3. When interpretability is important: Ensemble methods can be complex and difficult to interpret, especially when combining multiple models. If\n",
    "# interpretability is a key requirement, it may be better to use a simpler model that is easier to understand and explain.\n",
    "\n",
    "# 4. When computational resources are limited: Ensemble methods can be computationally expensive, especially when training multiple base models on large\n",
    "# datasets. If computational resources are limited, it may be challenging to implement and train ensemble models effectively.\n",
    "\n",
    "# In these cases, it may be better to use a single model or a simpler model that meets the specific requirements of the problem without the need for\n",
    "# ensemble learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. How does Bagging help in reducing overfitting?\n",
    "\n",
    "# Answer:- Bagging (Bootstrap Aggregating) helps in reducing overfitting by training multiple base models on different subsets of the training data and\n",
    "# combining their predictions to make the final prediction. The key ways in which Bagging reduces overfitting are:\n",
    "\n",
    "# 1. Variance reduction: By training multiple base models on different subsets of the training data, Bagging reduces the variance of the model by\n",
    "# averaging out the individual model's errors. This helps to create a more stable and robust model that generalizes well to unseen data.\n",
    "\n",
    "# 2. Model diversity: Bagging introduces randomness by using bootstrapped samples of the training data and random subsets of features at each split\n",
    "# in the decision tree. This randomness helps to create diverse base models that capture different patterns in the data, reducing the correlation\n",
    "\n",
    "# between the models and improving the generalization ability of the model.\n",
    "\n",
    "# 3. Out-of-Bag estimation: Bagging provides an unbiased estimate of the model's performance on unseen data using the out-of-bag samples. This allows\n",
    "# for tuning hyperparameters and evaluating the model's generalization ability without the need for a separate validation set.\n",
    "\n",
    "# By reducing overfitting, Bagging helps to create more accurate and reliable models that can make better predictions on new and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Why is Random Forest better than a single Decision Tree?\n",
    "\n",
    "# Answer:- Random Forest is better than a single Decision Tree for several reasons:\n",
    "\n",
    "# 1. Random Forest reduces overfitting: Random Forest combines multiple decision trees trained on different subsets of the data, which helps to reduce\n",
    "# overfitting and improve the generalization ability of the model. By averaging the predictions of multiple trees, Random Forest can make more accurate\n",
    "# predictions on unseen data.\n",
    "\n",
    "# 2. Random Forest is more robust: Random Forest is less sensitive to noise and outliers in the data compared to a single Decision Tree. By aggregating\n",
    "# the predictions of multiple trees, Random Forest can handle noisy data and make more reliable predictions.\n",
    "\n",
    "# 3. Random Forest captures more complex patterns: Random Forest can capture more complex patterns in the data by combining the predictions of multiple\n",
    "# trees. The randomness introduced by selecting a random subset of features at each split helps to create diverse trees that capture different aspects\n",
    "# of the data.\n",
    "\n",
    "# 4. Random Forest provides feature importance: Random Forest can provide information about the importance of features in predicting the target variable.\n",
    "# This can help to identify the most relevant features in the data and gain insights into the underlying patterns.\n",
    "\n",
    "# Overall, Random Forest is a powerful and versatile ensemble learning method that can outperform a single Decision Tree in terms of predictive performance\n",
    "# and generalization ability.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. What is the role of bootstrap sampling in Bagging?\n",
    "\n",
    "# Answer:- Bootstrap sampling is a key component of Bagging (Bootstrap Aggregating) that helps to create diverse subsets of the training data for training\n",
    "# multiple base models. The role of bootstrap sampling in Bagging is as follows:\n",
    "\n",
    "# 1. Generate B bootstrap samples: In Bagging, B bootstrap samples are generated by randomly sampling N instances with replacement from the original training\n",
    "# data set, where N is the total number of instances in the training data set. Each bootstrap sample has the same size as the original data set but may\n",
    "# contain duplicate instances.\n",
    "\n",
    "# 2. Train base models on bootstrap samples: A base model is trained on each bootstrap sample to get B different base models. Each base model captures\n",
    "# different patterns in the data due to the randomness introduced by bootstrap sampling.\n",
    "\n",
    "# 3. Combine predictions: The predictions of the B base models are combined using a majority voting (for classification) or averaging (for regression) to\n",
    "# make the final prediction. By combining the predictions of multiple base models, Bagging reduces overfitting and improves the generalization ability of\n",
    "# the model.\n",
    "\n",
    "# Bootstrap sampling helps to create diverse subsets of the training data, reduce the variance of the model, and improve the predictive performance of the\n",
    "# ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. What are some real-world applications of ensemble techniques?\n",
    "\n",
    "# Answer:- Ensemble techniques are widely used in various real-world applications across different domains. Some common real-world applications of ensemble\n",
    "# techniques include:\n",
    "\n",
    "# 1. Credit scoring: Ensemble techniques are used in credit scoring to predict the creditworthiness of individuals based on their financial history and\n",
    "# other relevant factors. Ensemble models can combine the predictions of multiple base models to make more accurate credit risk assessments.\n",
    "\n",
    "# 2. Fraud detection: Ensemble techniques are used in fraud detection systems to identify fraudulent transactions or activities. By combining the predictions\n",
    "# of multiple base models, ensemble models can improve the detection of fraudulent behavior and reduce false positives.\n",
    "\n",
    "# 3. Medical diagnosis: Ensemble techniques are used in medical diagnosis to predict the presence of diseases or conditions based on patient data and\n",
    "# diagnostic tests. Ensemble models can combine the predictions of multiple base models to make more accurate and reliable diagnoses.\n",
    "\n",
    "# 4. Stock market prediction: Ensemble techniques are used in stock market prediction to forecast stock prices and trends. By combining the predictions of\n",
    "# multiple base models, ensemble models can provide more accurate and reliable predictions of stock market movements.\n",
    "\n",
    "# 5. Natural language processing: Ensemble techniques are used in natural language processing tasks such as sentiment analysis, text classification, and\n",
    "# machine translation. Ensemble models can combine the predictions of multiple base models to improve the accuracy and robustness of language processing\n",
    "# tasks.\n",
    "\n",
    "# These are just a few examples of the many real-world applications of ensemble techniques. Ensemble methods are versatile and can be applied to a wide\n",
    "# range of machine learning tasks to improve predictive performance and generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. What is the difference between Bagging and Boosting?\n",
    "\n",
    "# Answer:- Bagging and Boosting are two popular ensemble learning methods that combine the predictions of multiple base models to create a more accurate\n",
    "# and robust model. The main differences between Bagging and Boosting are as follows:\n",
    "\n",
    "# 1. Training process: In Bagging, multiple base models are trained independently on different subsets of the training data, and the predictions are\n",
    "# combined using a majority voting (for classification) or averaging (for regression) to make the final prediction. In Boosting, base models are trained\n",
    "# sequentially, where each subsequent model focuses on the instances that were misclassified by the previous models.\n",
    "\n",
    "# 2. Model diversity: Bagging aims to reduce variance by creating diverse base models that capture different patterns in the data. Boosting aims to reduce\n",
    "# bias by focusing on the instances that are difficult to classify, which can lead to higher model complexity and overfitting.\n",
    "\n",
    "# 3. Weighting of instances: In Bagging, each base model is trained on a random subset of the training data with replacement, and all instances have equal\n",
    "# weight. In Boosting, the instances are weighted based on their importance, with more weight given to instances that are difficult to classify.\n",
    "\n",
    "# 4. Final prediction: In Bagging, the final prediction is made by aggregating the predictions of multiple base models. In Boosting, the final prediction\n",
    "# is made by combining the predictions of all base models using a weighted sum.\n",
    "\n",
    "# Overall, Bagging and Boosting are complementary ensemble learning methods that have different training processes and objectives. Bagging focuses on\n",
    "# reducing variance and creating diverse base models, while Boosting focuses on reducing bias and improving the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy:0.93\n"
     ]
    }
   ],
   "source": [
    "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred=bagging_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(f\"Bagging Classifier Accuracy:{acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regession Mean Squared Error:3880.77%\n"
     ]
    }
   ],
   "source": [
    "# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred=bagging_reg.predict(X_test)\n",
    "\n",
    "MSE=mean_squared_error(y_test,y_pred)\n",
    "\n",
    "print(f\"Bagging Regession Mean Squared Error:{MSE:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Importance\n",
      "7       mean concave points    0.141752\n",
      "23               worst area    0.132649\n",
      "22          worst perimeter    0.129879\n",
      "20             worst radius    0.105420\n",
      "27     worst concave points    0.091248\n",
      "13               area error    0.065114\n",
      "3                 mean area    0.061547\n",
      "2            mean perimeter    0.035126\n",
      "6            mean concavity    0.028960\n",
      "12          perimeter error    0.021440\n",
      "26          worst concavity    0.019856\n",
      "21            worst texture    0.019795\n",
      "5          mean compactness    0.017279\n",
      "25        worst compactness    0.015556\n",
      "0               mean radius    0.014834\n",
      "24         worst smoothness    0.013220\n",
      "10             radius error    0.012796\n",
      "1              mean texture    0.012307\n",
      "28           worst symmetry    0.009642\n",
      "29  worst fractal dimension    0.007247\n",
      "17     concave points error    0.006690\n",
      "11            texture error    0.005965\n",
      "4           mean smoothness    0.005442\n",
      "16          concavity error    0.004690\n",
      "18           symmetry error    0.004424\n",
      "9    mean fractal dimension    0.004356\n",
      "15        compactness error    0.004148\n",
      "14         smoothness error    0.003465\n",
      "19  fractal dimension error    0.002985\n",
      "8             mean symmetry    0.002167\n"
     ]
    }
   ],
   "source": [
    "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_clf=RandomForestClassifier(criterion=\"gini\")\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=rf_clf.predict(X_test)\n",
    "\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "feature_names = load_breast_cancer().feature_names\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Mean Squared Error:0.11\n",
      "Random Forest Mean Squared Error:0.08\n"
     ]
    }
   ],
   "source": [
    "# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "X,y=load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "dt_reg=DecisionTreeRegressor()\n",
    "dt_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred_dt=dt_reg.predict(X_test)\n",
    "\n",
    "MSE_dt=mean_squared_error(y_test,y_pred_dt)\n",
    "\n",
    "rf_reg=RandomForestRegressor()\n",
    "rf_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf=rf_reg.predict(X_test)\n",
    "\n",
    "MSE_rf=mean_squared_error(y_test,y_pred_rf)\n",
    "\n",
    "print(f\"Decision Tree Mean Squared Error:{MSE_dt:.2f}\")\n",
    "print(f\"Random Forest Mean Squared Error:{MSE_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy:0.96\n",
      "Random Forest Classifier OOB Score:0.96\n"
     ]
    }
   ],
   "source": [
    "# 25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_clf=RandomForestClassifier(criterion=\"gini\",oob_score=True)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=rf_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "\n",
    "oob_score=rf_clf.oob_score_\n",
    "\n",
    "print(f\"Random Forest Classifier Accuracy:{acc:.2f}\")\n",
    "print(f\"Random Forest Classifier OOB Score:{oob_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Bagging Classifier: 0.84\n"
     ]
    }
   ],
   "source": [
    "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X,y=make_classification(n_samples=1000,n_features=4,n_classes=2,random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "svc_clf=SVC(C=1,kernel=\"linear\",degree=4)\n",
    "bagging_clf=BaggingClassifier(estimator=svc_clf)\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=bagging_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy of Bagging Classifier: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomforest with 10 trees -Accuracy:0.94\n",
      "Randomforest with 20 trees -Accuracy:0.94\n",
      "Randomforest with 40 trees -Accuracy:0.96\n",
      "Randomforest with 100 trees -Accuracy:0.95\n",
      "\n",
      "Accuracy results for different numbers of trees:\n",
      "10 trees: 0.94\n",
      "20 trees: 0.94\n",
      "40 trees: 0.96\n",
      "100 trees: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "n_trees=[10,20,40,100]\n",
    "\n",
    "acc_score={}\n",
    "\n",
    "for n in n_trees:\n",
    "    rf_clf=RandomForestClassifier(n_estimators=n,criterion=\"gini\")\n",
    "    rf_clf.fit(X_train,y_train)\n",
    "    y_pred=rf_clf.predict(X_test)\n",
    "    acc=accuracy_score(y_test,y_pred)\n",
    "    acc_score[n]=acc\n",
    "    print (f\"Randomforest with {n} trees -Accuracy:{acc:.2f}\")\n",
    "    \n",
    "    \n",
    "print(\"\\nAccuracy results for different numbers of trees:\")\n",
    "for n,acc in acc_score.items():\n",
    "    print(f\"{n} trees: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score of Bagging Classifier: 0.92\n"
     ]
    }
   ],
   "source": [
    "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X,y=make_classification(n_samples=1000,n_features=4,n_classes=2,random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "lr_clf=LogisticRegression()\n",
    "bagging_clf=BaggingClassifier(estimator=lr_clf)\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_proba=bagging_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "auc=roc_auc_score(y_test,y_pred_proba)\n",
    "print(f\"ROC-AUC Score of Bagging Classifier: {auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature  Importance\n",
      "3  Feature 3    0.686509\n",
      "0  Feature 0    0.146629\n",
      "2  Feature 2    0.086398\n",
      "1  Feature 1    0.080464\n"
     ]
    }
   ],
   "source": [
    "# 29. Train a Random Forest Regressor and analyze feature importance scores.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_reg=RandomForestRegressor()\n",
    "rf_reg.fit(X_train,y_train)\n",
    "\n",
    "y_pred=rf_reg.predict(X_test)\n",
    "\n",
    "feature_importances = rf_reg.feature_importances_\n",
    "\n",
    "feature_names = [f'Feature {i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy:0.91\n",
      "Random Forest Classifier Accuracy:0.95\n"
     ]
    }
   ],
   "source": [
    "# 30.  Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "bagging_clf=BaggingClassifier()\n",
    "rf_clf=RandomForestClassifier()\n",
    "\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_bagging=bagging_clf.predict(X_test)\n",
    "y_pred_rf=rf_clf.predict(X_test)\n",
    "\n",
    "acc_bagging=accuracy_score(y_test,y_pred_bagging)\n",
    "acc_rf=accuracy_score(y_test,y_pred_rf)\n",
    "\n",
    "print(f\"Bagging Classifier Accuracy:{acc_bagging:.2f}\")\n",
    "print(f\"Random Forest Classifier Accuracy:{acc_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.95\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END criterion=gini, max_depth=1, max_features=sqrt;, score=0.912 total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, max_features=sqrt;, score=0.963 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, max_features=sqrt;, score=0.887 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, max_features=sqrt;, score=0.924 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, max_features=sqrt;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=1, max_features=log2;, score=0.887 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=1, max_features=log2;, score=0.963 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=1, max_features=log2;, score=0.887 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=1, max_features=log2;, score=0.924 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=1, max_features=log2;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=sqrt;, score=0.938 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=sqrt;, score=1.000 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=sqrt;, score=0.988 total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=sqrt;, score=0.949 total time=   0.4s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=sqrt;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=4, max_features=log2;, score=0.938 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=4, max_features=log2;, score=0.988 total time=   0.2s\n",
      "[CV 3/5] END criterion=gini, max_depth=4, max_features=log2;, score=0.950 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=4, max_features=log2;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=4, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, max_features=sqrt;, score=1.000 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, max_features=sqrt;, score=0.975 total time=   0.2s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=6, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=6, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=6, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=6, max_features=log2;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=6, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=sqrt;, score=0.950 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=9, max_features=log2;, score=0.950 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=9, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=9, max_features=log2;, score=0.950 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=9, max_features=log2;, score=0.949 total time=   0.2s\n",
      "[CV 5/5] END criterion=gini, max_depth=9, max_features=log2;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=sqrt;, score=0.950 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=10, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=10, max_features=log2;, score=1.000 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=10, max_features=log2;, score=0.963 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=10, max_features=log2;, score=0.962 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=10, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=gini, max_depth=50, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=gini, max_depth=50, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=50, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=50, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=50, max_features=sqrt;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=gini, max_depth=50, max_features=log2;, score=0.938 total time=   0.2s\n",
      "[CV 2/5] END criterion=gini, max_depth=50, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=gini, max_depth=50, max_features=log2;, score=0.950 total time=   0.1s\n",
      "[CV 4/5] END criterion=gini, max_depth=50, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=gini, max_depth=50, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, max_features=sqrt;, score=0.912 total time=   0.6s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, max_features=sqrt;, score=0.887 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, max_features=sqrt;, score=0.924 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, max_features=sqrt;, score=0.937 total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1, max_features=log2;, score=0.925 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1, max_features=log2;, score=0.963 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1, max_features=log2;, score=0.887 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1, max_features=log2;, score=0.937 total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=sqrt;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=4, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=4, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=4, max_features=log2;, score=0.963 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=4, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=4, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, max_features=sqrt;, score=0.938 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, max_features=sqrt;, score=0.949 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=6, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=6, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=6, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=6, max_features=log2;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=6, max_features=log2;, score=0.949 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, max_features=sqrt;, score=0.925 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, max_features=sqrt;, score=0.988 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=9, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=9, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=9, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=9, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=9, max_features=log2;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, max_features=sqrt;, score=0.950 total time=   0.2s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=10, max_features=log2;, score=0.938 total time=   0.2s\n",
      "[CV 2/5] END criterion=entropy, max_depth=10, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=10, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=10, max_features=log2;, score=0.937 total time=   0.2s\n",
      "[CV 5/5] END criterion=entropy, max_depth=10, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=50, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=50, max_features=sqrt;, score=0.988 total time=   0.2s\n",
      "[CV 3/5] END criterion=entropy, max_depth=50, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=50, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=50, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=50, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=entropy, max_depth=50, max_features=log2;, score=1.000 total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=50, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=50, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=50, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1, max_features=sqrt;, score=0.900 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1, max_features=sqrt;, score=0.963 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1, max_features=sqrt;, score=0.900 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1, max_features=sqrt;, score=0.911 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1, max_features=sqrt;, score=0.924 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1, max_features=log2;, score=0.912 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1, max_features=log2;, score=0.887 total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1, max_features=log2;, score=0.924 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1, max_features=log2;, score=0.924 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=4, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=4, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=4, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=4, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=4, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=4, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=4, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=4, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=4, max_features=log2;, score=0.949 total time=   0.2s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=4, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=6, max_features=sqrt;, score=0.950 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=6, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=6, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=6, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=6, max_features=sqrt;, score=0.937 total time=   0.2s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=6, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=6, max_features=log2;, score=1.000 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=6, max_features=log2;, score=0.963 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=6, max_features=log2;, score=0.962 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=6, max_features=log2;, score=0.949 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=9, max_features=sqrt;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=9, max_features=sqrt;, score=1.000 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=9, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=9, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=9, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=9, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=9, max_features=log2;, score=1.000 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=9, max_features=log2;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=9, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=9, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=10, max_features=sqrt;, score=0.963 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=10, max_features=sqrt;, score=0.988 total time=   0.2s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=10, max_features=sqrt;, score=0.975 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=10, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=10, max_features=sqrt;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=10, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=10, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=10, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=10, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=10, max_features=log2;, score=0.937 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=50, max_features=sqrt;, score=0.950 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=50, max_features=sqrt;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=50, max_features=sqrt;, score=0.963 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=50, max_features=sqrt;, score=0.949 total time=   0.2s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=50, max_features=sqrt;, score=0.949 total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=50, max_features=log2;, score=0.938 total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=50, max_features=log2;, score=0.988 total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=50, max_features=log2;, score=0.963 total time=   0.1s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=50, max_features=log2;, score=0.949 total time=   0.1s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=50, max_features=log2;, score=0.937 total time=   0.1s\n",
      "----------------------------------------------------------------------\n",
      "Best parameters found:  {'criterion': 'log_loss', 'max_depth': 6, 'max_features': 'log2'}\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Classifier Accuracy: {acc_rf:.2f}\")\n",
    "\n",
    "params = {\n",
    "    \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "    \"max_depth\": [1, 4, 6, 9, 10, 50],\n",
    "    # \"min_samples_split\": [10, 20, 30, 50],\n",
    "    # \"min_samples_leaf\": [1, 2, 3, 6, 8, 9],\n",
    "    # \"min_weight_fraction_leaf\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"max_features\": ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=params, cv=5, verbose=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"-------\"*10)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"-------\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of estimators: 1,R-squared:0.96\n",
      "number of estimators: 2,R-squared:0.98\n",
      "number of estimators: 3,R-squared:0.99\n",
      "number of estimators: 5,R-squared:0.98\n",
      "number of estimators: 6,R-squared:0.99\n",
      "number of estimators: 8,R-squared:0.99\n",
      "number of estimators: 9,R-squared:0.99\n",
      "number of estimators: 10,R-squared:0.99\n"
     ]
    }
   ],
   "source": [
    "# 32.Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X,y=make_regression(n_samples=1000,n_features=4,n_informative=2,noise=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "n_estimators=[1,2,3,5,6,8,9,10]\n",
    "r2Score=[]\n",
    "\n",
    "for n in n_estimators:\n",
    "    bagging_reg=BaggingRegressor(n_estimators=n)\n",
    "    bagging_reg.fit(X_train,y_train)\n",
    "    y_pred=bagging_reg.predict(X_test)\n",
    "    r2=r2_score(y_test,y_pred)\n",
    "    r2Score.append((n,r2))\n",
    "\n",
    "for n,r2 in r2Score:\n",
    "    print(f\"number of estimators: {n},R-squared:{r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy:0.98\n",
      "Misclassified Samples  : [[6.7 3.  5.  1.7]]\n",
      "True Labels of Misclassified Samples  :[1]\n",
      "Predicted Labels of Misclassified Samples  :[2]\n"
     ]
    }
   ],
   "source": [
    "# 33. Train a Random Forest Classifier and analyze misclassified samples.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y=load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_clf=RandomForestClassifier(criterion=\"gini\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "rf_clf.fit(X_train,y_train)\n",
    "y_pred=rf_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy:{acc:.2f}\")\n",
    "\n",
    "misclassified_samples=X_test[y_test!=y_pred]\n",
    "misclassified_labels=y_test[y_test!=y_pred]\n",
    "misclassified_predictions=y_pred[y_test!=y_pred]\n",
    "\n",
    "print(f\"Misclassified Samples  : {misclassified_samples}\")\n",
    "print(f\"True Labels of Misclassified Samples  :{misclassified_labels}\")\n",
    "print(f\"Predicted Labels of Misclassified Samples  :{misclassified_predictions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy:0.92\n",
      "Decision Tree Classifier Accuracy:0.92\n"
     ]
    }
   ],
   "source": [
    "# 34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "bagging_clf=BaggingClassifier()\n",
    "dt_clf=DecisionTreeClassifier()\n",
    "\n",
    "bagging_clf.fit(X_train,y_train)\n",
    "dt_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_bagging=bagging_clf.predict(X_test)\n",
    "y_pred_dt=dt_clf.predict(X_test)\n",
    "\n",
    "acc_bagging=accuracy_score(y_test,y_pred_bagging)\n",
    "acc_dt=accuracy_score(y_test,y_pred_dt)\n",
    "\n",
    "print(f\"Bagging Classifier Accuracy:{acc_bagging:.2f}\")\n",
    "print(f\"Decision Tree Classifier Accuracy:{acc_dt:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy:0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 35. Train a Random Forest Classifier and visualize the confusion matrix.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X,y=load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_clf=RandomForestClassifier(criterion=\"gini\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "rf_clf.fit(X_train,y_train)\n",
    "y_pred=rf_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(f\"Random Forest Classifier Accuracy:{acc:.2f}\")\n",
    "\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm,annot=True,fmt='d', cmap='Blues', xticklabels=load_iris().target_names, yticklabels=load_iris().target_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "estimator=[\n",
    "    (\"Decision Tree\",DecisionTreeClassifier()),\n",
    "    (\"SVM\",SVC(probability=True)),\n",
    "    (\"Logistic Regression\",LogisticRegression())\n",
    "]\n",
    "stacking_clf=StackingClassifier(estimators=estimator,final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train,y_train)\n",
    "y_pred=stacking_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(f\"Stacking Classifier Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Feature  Importance\n",
      "23            worst area    0.124333\n",
      "22       worst perimeter    0.122479\n",
      "20          worst radius    0.116986\n",
      "7    mean concave points    0.109505\n",
      "27  worst concave points    0.098626\n"
     ]
    }
   ],
   "source": [
    "# 37. Train a Random Forest Classifier and print the top 5 most important features.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "rf_clf=RandomForestClassifier(criterion=\"gini\")\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=rf_clf.predict(X_test)\n",
    "\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "feature_names = load_breast_cancer().feature_names\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy:0.93\n",
      "Bagging Classifier Precision:0.92\n",
      "Bagging Classifier Recall:0.92\n",
      "Bagging Classifier F1-Score:0.92\n"
     ]
    }
   ],
   "source": [
    "# 38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "bagging_clf = BaggingClassifier()\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred=bagging_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "precision=precision_score(y_test,y_pred, average='macro')\n",
    "recall=recall_score(y_test,y_pred, average='macro')\n",
    "f1=f1_score(y_test,y_pred, average='macro')\n",
    "\n",
    "print(f\"Bagging Classifier Accuracy:{acc:.2f}\")\n",
    "print(f\"Bagging Classifier Precision:{precision:.2f}\")\n",
    "print(f\"Bagging Classifier Recall:{recall:.2f}\")\n",
    "print(f\"Bagging Classifier F1-Score:{f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of Max Depth 1 Accuracy: 0.93\n",
      "Numbers of Max Depth 2 Accuracy: 0.94\n",
      "Numbers of Max Depth 3 Accuracy: 0.95\n",
      "Numbers of Max Depth 4 Accuracy: 0.95\n",
      "Numbers of Max Depth 5 Accuracy: 0.96\n",
      "Numbers of Max Depth 6 Accuracy: 0.96\n",
      "Numbers of Max Depth 7 Accuracy: 0.95\n",
      "Numbers of Max Depth 8 Accuracy: 0.96\n",
      "Numbers of Max Depth 9 Accuracy: 0.95\n",
      "Numbers of Max Depth 10 Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "max_depth=[1,2,3,4,5,6,7,8,9,10]\n",
    "results=[]\n",
    "\n",
    "for n in max_depth:\n",
    "    rf_clf=RandomForestClassifier(criterion=\"gini\",max_depth=n)\n",
    "    rf_clf.fit(X_train,y_train)\n",
    "    y_pred=rf_clf.predict(X_test)\n",
    "    acc=accuracy_score(y_test,y_pred)\n",
    "    results.append((n,acc))\n",
    "    \n",
    "for n,acc in results:\n",
    "    print(f\"Numbers of Max Depth {n} Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regressor MSE:0.04, R2 Score:0.82\n",
      "KNN Regressor MSE:0.07, R2 Score:0.71\n"
     ]
    }
   ],
   "source": [
    "# 40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "knn=KNeighborsRegressor()\n",
    "dt_reg=DecisionTreeRegressor()\n",
    "\n",
    "bagging_reg_knn=BaggingRegressor(estimator=knn)\n",
    "bagging_reg_dt=BaggingRegressor(estimator=dt_reg)\n",
    "\n",
    "\n",
    "bagging_reg_knn.fit(X_train,y_train)\n",
    "bagging_reg_dt.fit(X_train,y_train)\n",
    "\n",
    "y_pred_bagging_knn=bagging_reg_knn.predict(X_test)\n",
    "y_pred_bagging_dt=bagging_reg_dt.predict(X_test)\n",
    "\n",
    "mse_bagging_knn=mean_squared_error(y_test,y_pred_bagging_knn)\n",
    "mse_bagging_dt=mean_squared_error(y_test,y_pred_bagging_dt)\n",
    "\n",
    "r2_bagging_knn=r2_score(y_test,y_pred_bagging_knn)\n",
    "r2_bagging_dt=r2_score(y_test,y_pred_bagging_dt)\n",
    "\n",
    "print(f\"Decision Tree Regressor MSE:{mse_bagging_dt:.2f}, R2 Score:{r2_bagging_dt:.2f}\")\n",
    "print(f\"KNN Regressor MSE:{mse_bagging_knn:.2f}, R2 Score:{r2_bagging_knn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier ROC-AUC Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# 41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = rf_clf.predict_proba(X_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy:0.93\n",
      "Cross-Validation Scores: [0.96666667 0.96666667 0.9        0.9        1.        ]\n",
      "Mean Cross-Validation Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "# 42. Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "bagging_clf = BaggingClassifier()\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred=bagging_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "\n",
    "\n",
    "print(f\"Bagging Classifier Accuracy:{acc:.2f}\")\n",
    "\n",
    "bagging_clf = BaggingClassifier()\n",
    "scores = cross_val_score(bagging_clf, X, y, cv=5)\n",
    "\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Mean Cross-Validation Score: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAJwCAYAAAC+pzHoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABwXUlEQVR4nO3dd3hUVf7H8fekJ0AEpIoRVFQsiArCgiIWurJiZUERsayuYmNt7KpgxcqiLoprQ3dxQWyLiigiWAAbCj8rIopYqArSSbu/P4aEDAlMBpJMEt6v58kjc+45M9+ZHGI+nHvPDQVBECBJkiRJ2qaEeBcgSZIkSZWdwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkrRd5557Ls2aNYtpzPTp0wmFQkyfPr1caqrqjj32WI499tjCxwsXLiQUCjFmzJi41SRJ2j6DkyRVMmPGjCEUChV+paWlsf/++zNo0CCWLl0a7/IqvYIQUvCVkJBA3bp16dGjB7NmzYp3eWVi6dKlXH311bRo0YKMjAxq1KhB69atue2221i1alW8y5Okaikp3gVIkkp2yy23sPfee7Nx40bee+89Hn74YSZNmsTnn39ORkZGhdXx6KOPkp+fH9OYY445hg0bNpCSklJOVUXXt29fevbsSV5eHt988w0PPfQQxx13HB999BEtW7aMW10766OPPqJnz56sXbuWs88+m9atWwPw8ccfc+edd/LOO+/wxhtvxLlKSap+DE6SVEn16NGDNm3aAHDBBRew++67M2LECP73v//Rt2/fEsesW7eOGjVqlGkdycnJMY9JSEggLS2tTOuI1RFHHMHZZ59d+Lhjx4706NGDhx9+mIceeiiOle24VatWccopp5CYmMinn35KixYtIo7ffvvtPProo2XyWuUxlySpKvNUPUmqIo4//ngAvv/+eyB87VHNmjVZsGABPXv2pFatWpx11lkA5OfnM3LkSA4++GDS0tJo2LAhF110EStXriz2vK+99hqdOnWiVq1aZGZmcuSRR/LMM88UHi/pGqdx48bRunXrwjEtW7bk/vvvLzy+rWucJkyYQOvWrUlPT6devXqcffbZ/PzzzxF9Ct7Xzz//TO/evalZsyb169fn6quvJi8vb4c/v44dOwKwYMGCiPZVq1Zx5ZVXkpWVRWpqKs2bN+euu+4qtsqWn5/P/fffT8uWLUlLS6N+/fp0796djz/+uLDPk08+yfHHH0+DBg1ITU3loIMO4uGHH97hmrf2yCOP8PPPPzNixIhioQmgYcOG3HDDDYWPQ6EQw4YNK9avWbNmnHvuuYWPC04Pffvtt7nkkkto0KABe+65J88991xhe0m1hEIhPv/888K2r7/+mtNPP526deuSlpZGmzZtmDhx4s69aUmqJFxxkqQqouAX/t13372wLTc3l27dunH00Udz7733Fp7Cd9FFFzFmzBgGDhzI5Zdfzvfff88///lPPv30U2bMmFG4ijRmzBjOO+88Dj74YIYMGULt2rX59NNPmTx5Mv369SuxjilTptC3b19OOOEE7rrrLgC++uorZsyYwRVXXLHN+gvqOfLIIxk+fDhLly7l/vvvZ8aMGXz66afUrl27sG9eXh7dunWjXbt23Hvvvbz55pvcd9997LvvvvzlL3/Zoc9v4cKFANSpU6ewbf369XTq1Imff/6Ziy66iL322ouZM2cyZMgQFi9ezMiRIwv7nn/++YwZM4YePXpwwQUXkJuby7vvvsv7779fuDL48MMPc/DBB/PHP/6RpKQkXn75ZS655BLy8/O59NJLd6juoiZOnEh6ejqnn376Tj9XSS655BLq16/PTTfdxLp16zjxxBOpWbMmzz77LJ06dYroO378eA4++GAOOeQQAL744guOOuoomjRpwvXXX0+NGjV49tln6d27N88//zynnHJKudQsSRUmkCRVKk8++WQABG+++WawfPny4McffwzGjRsX7L777kF6enrw008/BUEQBAMGDAiA4Prrr48Y/+677wZAMHbs2Ij2yZMnR7SvWrUqqFWrVtCuXbtgw4YNEX3z8/ML/zxgwICgadOmhY+vuOKKIDMzM8jNzd3me5g2bVoABNOmTQuCIAiys7ODBg0aBIccckjEa73yyisBENx0000RrwcEt9xyS8RzHn744UHr1q23+ZoFvv/++wAIbr755mD58uXBkiVLgnfffTc48sgjAyCYMGFCYd9bb701qFGjRvDNN99EPMf1118fJCYmBosWLQqCIAjeeuutAAguv/zyYq9X9LNav359sePdunUL9tlnn4i2Tp06BZ06dSpW85NPPrnd91anTp2gVatW2+1TFBAMHTq0WHvTpk2DAQMGFD4umHNHH310se9r3759gwYNGkS0L168OEhISIj4Hp1wwglBy5Ytg40bNxa25efnBx06dAj222+/UtcsSZWVp+pJUiXVuXNn6tevT1ZWFn/605+oWbMmL774Ik2aNInot/UKzIQJE9htt93o0qULK1asKPxq3bo1NWvWZNq0aUB45WjNmjVcf/31xa5HCoVC26yrdu3arFu3jilTppT6vXz88ccsW7aMSy65JOK1TjzxRFq0aMGrr75abMzFF18c8bhjx4589913pX7NoUOHUr9+fRo1akTHjh356quvuO+++yJWayZMmEDHjh2pU6dOxGfVuXNn8vLyeOeddwB4/vnnCYVCDB06tNjrFP2s0tPTC//8+++/s2LFCjp16sR3333H77//Xurat2X16tXUqlVrp59nWy688EISExMj2vr06cOyZcsiTrt87rnnyM/Pp0+fPgD89ttvvPXWW5x55pmsWbOm8HP89ddf6datG/Pnzy92SqYkVTWeqidJldSoUaPYf//9SUpKomHDhhxwwAEkJET+e1dSUhJ77rlnRNv8+fP5/fffadCgQYnPu2zZMmDLqX8Fp1qV1iWXXMKzzz5Ljx49aNKkCV27duXMM8+ke/fu2xzzww8/AHDAAQcUO9aiRQvee++9iLaCa4iKqlOnTsQ1WsuXL4+45qlmzZrUrFmz8PGf//xnzjjjDDZu3Mhbb73FAw88UOwaqfnz5/N///d/xV6rQNHPao899qBu3brbfI8AM2bMYOjQocyaNYv169dHHPv999/Zbbfdtjs+mszMTNasWbNTz7E9e++9d7G27t27s9tuuzF+/HhOOOEEIHya3mGHHcb+++8PwLfffksQBNx4443ceOONJT73smXLioV+SapKDE6SVEm1bdu28NqZbUlNTS0WpvLz82nQoAFjx44tccy2QkJpNWjQgDlz5vD666/z2muv8dprr/Hkk09yzjnn8NRTT+3UcxfYetWjJEceeWRhIIPwClPRjRD2228/OnfuDMBJJ51EYmIi119/Pccdd1zh55qfn0+XLl249tprS3yNgmBQGgsWLOCEE06gRYsWjBgxgqysLFJSUpg0aRL/+Mc/Yt7SvSQtWrRgzpw5ZGdn79RW79vaZKPoilmB1NRUevfuzYsvvshDDz3E0qVLmTFjBnfccUdhn4L3dvXVV9OtW7cSn7t58+Y7XK8kVQYGJ0mqZvbdd1/efPNNjjrqqBJ/ES7aD+Dzzz+P+ZfalJQUevXqRa9evcjPz+eSSy7hkUce4cYbbyzxuZo2bQrAvHnzCncHLDBv3rzC47EYO3YsGzZsKHy8zz77bLf/3//+dx599FFuuOEGJk+eDIQ/g7Vr1xYGrG3Zd999ef311/ntt9+2uer08ssvs2nTJiZOnMhee+1V2F5wamRZ6NWrF7NmzeL555/f5pb0RdWpU6fYDXGzs7NZvHhxTK/bp08fnnrqKaZOncpXX31FEASFp+nBls8+OTk56mcpSVWV1zhJUjVz5plnkpeXx6233lrsWG5ubuEv0l27dqVWrVoMHz6cjRs3RvQLgmCbz//rr79GPE5ISODQQw8FYNOmTSWOadOmDQ0aNGD06NERfV577TW++uorTjzxxFK9t6KOOuooOnfuXPgVLTjVrl2biy66iNdff505c+YA4c9q1qxZvP7668X6r1q1itzcXABOO+00giDg5ptvLtav4LMqWCUr+tn9/vvvPPnkkzG/t225+OKLady4MX/961/55ptvih1ftmwZt912W+Hjfffdt/A6rQL/+te/Yt7WvXPnztStW5fx48czfvx42rZtG3FaX4MGDTj22GN55JFHSgxly5cvj+n1JKkycsVJkqqZTp06cdFFFzF8+HDmzJlD165dSU5OZv78+UyYMIH777+f008/nczMTP7xj39wwQUXcOSRR9KvXz/q1KnD3LlzWb9+/TZPu7vgggv47bffOP7449lzzz354YcfePDBBznssMM48MADSxyTnJzMXXfdxcCBA+nUqRN9+/Yt3I68WbNmXHXVVeX5kRS64oorGDlyJHfeeSfjxo3jmmuuYeLEiZx00kmce+65tG7dmnXr1vHZZ5/x3HPPsXDhQurVq8dxxx1H//79eeCBB5g/fz7du3cnPz+fd999l+OOO45BgwbRtWvXwpW4iy66iLVr1/Loo4/SoEGDmFd4tqVOnTq8+OKL9OzZk8MOO4yzzz6b1q1bA/DJJ5/w3//+l/bt2xf2v+CCC7j44os57bTT6NKlC3PnzuX111+nXr16Mb1ucnIyp556KuPGjWPdunXce++9xfqMGjWKo48+mpYtW3LhhReyzz77sHTpUmbNmsVPP/3E3Llzd+7NS1K8xXNLP0lScQVbQ3/00Ufb7TdgwICgRo0a2zz+r3/9K2jdunWQnp4e1KpVK2jZsmVw7bXXBr/88ktEv4kTJwYdOnQI0tPTg8zMzKBt27bBf//734jXKbod+XPPPRd07do1aNCgQZCSkhLstddewUUXXRQsXry4sM/W25EXGD9+fHD44YcHqampQd26dYOzzjqrcHv1aO9r6NChQWn+t1Wwtfc999xT4vFzzz03SExMDL799tsgCIJgzZo1wZAhQ4LmzZsHKSkpQb169YIOHToE9957b5CdnV04Ljc3N7jnnnuCFi1aBCkpKUH9+vWDHj16BLNnz474LA899NAgLS0taNasWXDXXXcFTzzxRAAE33//fWG/Hd2OvMAvv/wSXHXVVcH+++8fpKWlBRkZGUHr1q2D22+/Pfj9998L++Xl5QXXXXddUK9evSAjIyPo1q1b8O23325zO/LtzbkpU6YEQBAKhYIff/yxxD4LFiwIzjnnnKBRo0ZBcnJy0KRJk+Ckk04KnnvuuVK9L0mqzEJBsJ3zMSRJkiRJXuMkSZIkSdEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSotjlboCbn5/PL7/8Qq1atQiFQvEuR5IkSVKcBEHAmjVr2GOPPUhI2P6a0i4XnH755ReysrLiXYYkSZKkSuLHH39kzz333G6fXS441apVCwh/OJmZmXGuBnJycnjjjTfo2rUrycnJ8S5HlZzzRbFyzihWzhnFyjmjWFWmObN69WqysrIKM8L27HLBqeD0vMzMzEoTnDIyMsjMzIz7xFHl53xRrJwzipVzRrFyzihWlXHOlOYSHjeHkCRJkqQoDE6SJEmSFIXBSZIkSZKi2OWucZIkaWfk5eWRk5MT7zLKTU5ODklJSWzcuJG8vLx4l6MqwDmjWFX0nElOTiYxMXGnn8fgJElSKa1du5affvqJIAjiXUq5CYKARo0a8eOPP3q/Q5WKc0axqug5EwqF2HPPPalZs+ZOPY/BSZKkUsjLy+Onn34iIyOD+vXrV9tfEPPz81m7di01a9aMejNICZwzil1FzpkgCFi+fDk//fQT++23306tPBmcJEkqhZycHIIgoH79+qSnp8e7nHKTn59PdnY2aWlp/hKsUnHOKFYVPWfq16/PwoULycnJ2ang5OyWJCkG1XWlSZKqq7L6uW1wkiRJkqQoDE6SJEmSFIXBSZIklblQKMRLL71U5n2ruunTpxMKhVi1ahUAY8aMoXbt2nGtqaK99NJLNG/enMTERK688sqYx1fVz+zxxx+na9eu8S6j2pk8eTKHHXYY+fn55f5aBidJkqqxc889l1AoRCgUIiUlhebNm3PLLbeQm5tbrq+7ePFievToUeZ9d0azZs0KP4uMjAxatmzJY489Vu6vq0gXXXQRp59+Oj/++CO33nprvMuJ2aJFizjxxBPJyMigQYMGXHPNNVH/Pm3cuJEbb7yRoUOHVlCVFW/jxo1ceuml7L777tSsWZPTTjuNpUuXlnr8xRdfTCgUYuTIkRHtf/zjH9lrr71IS0ujcePG9O/fn19++aXwePfu3UlOTmbs2LFl9Va2yeAkSVI11717dxYvXsz8+fP561//yrBhw7jnnntK7JudnV0mr9moUSNSU1PLvO/OuuWWW1i8eDGff/45Z599NhdeeCGvvfZahbx2ZVFW3+MdsXbtWpYtW0a3bt3YY489qFWrVtxq2RF5eXmceOKJZGdnM3PmTJ566inGjBnDTTfdtN1xzz33HJmZmRx11FE79fqV+ebbV111FS+//DITJkzg7bff5pdffuHUU08t1dgXX3yR999/nz322KPYseOOO45nn32WefPm8fzzz7NgwQJOP/30iD7nnnsuDzzwQJm8j+0xOEmSVM2lpqbSqFEjmjZtyl/+8hc6d+7MxIkTgfAvHL179+b2229njz324MADDwTgxx9/5Mwzz6R27drUrVuXk08+mYULF0Y87xNPPMHBBx9MamoqjRs3ZtCgQYXHip5+l52dzaBBg2jcuDFpaWk0bdqU4cOHl9gX4LPPPuP4448nPT2d3XffnT//+c+sXbu28HhBzffeey+NGzdm991359JLLy3VL5W1atWiUaNG7LPPPlx33XXUrVuXKVOmFB5ftWoVF1xwAfXr1yczM5Pjjz+euXPnRjzHyy+/zJFHHklaWhr16tXjlFNOKTz273//mzZt2hS+Tr9+/Vi2bFnUurbnp59+om/fvtStW5caNWrQpk0bPvjgg4jPoqgrr7ySY489tvDxsccey6BBg7jyyiupV68e3bp1o1+/fvTp0ydiXE5ODvXq1ePpp58GwltGDx8+nL333pv09HRatWrFc889t91aV65cyTnnnEOdOnXIyMigR48ezJ8/HwifplgQlI4//nhCoRDTp08v8XlWrVrFRRddRMOGDUlLS+OQQw7hlVdeKbHvggULOPnkk2nYsCE1a9bkyCOP5M0334zo89BDD7HffvuRlpZGw4YNI37xfu6552jZsmXhfOvcuTPr1q0r8bXeeOMNvvzyS/7zn/9w2GGH0aNHD2699VZGjRq13UA6btw4evXqFdH20Ucf0aVLF+rVq8duu+1Gp06d+OSTTyL6hEIhHn74Yf74xz9So0YNbr/9dgD+97//ccQRR5CWlsY+++zDzTffHLHqNWLECFq2bEmNGjXIysrikksuifg7VNZ+//13Hn/8cUaMGMHxxx9P69atefLJJ5k5cybvv//+dsf+/PPPXHbZZYwdO5bk5ORix6+66ir+8Ic/0LRpUzp06MD111/P+++/H/H3vVevXnz88ccsWLCgzN9bUQYnSZJ2UBtgzzh8tdnJutPT0yN+yZs6dSrz5s1jypQpTJw4kZycHHr06EGtWrV49913mTFjBjVr1qR79+6F4x5++GEuvfRS/vznP/PZZ58xceJEmjdvXuLrPfDAA0ycOLHwX43Hjh1Ls2bNSuy7bt06unXrRp06dfjoo4+YMGECb775ZkQoA5g2bRoLFixg2rRphf/qP2bMmFJ/Bvn5+Tz//POsXLmSlJSUwvYzzjiDZcuW8dprrzF79myOOOIITjjhBH777TcAXn31VU455RR69uzJp59+ytSpU2nbtm3h+JycHG699Vbmzp3LSy+9xMKFCzn33HNLXdfW1q5dS6dOnfj555+ZOHEic+fO5dprr435eo6nnnqKlJQUZsyYwejRoznrrLN4+eWXI36Zfv3111m/fn1hEBw+fDhPP/00o0eP5osvvuCqq67i7LPP5u23397m65x77rl8/PHHTJw4kVmzZhEEAT179iQnJ4cOHTowb948AJ5//nkWL15Mhw4dij1Hfn4+PXr0YMaMGfznP//hyy+/5M4779zm/XfWrl1Lz549mTp1Kp9++indu3enV69eLFq0CICPP/6Yyy+/nFtuuYV58+YxefJkjjnmGCB8mmjfvn0577zz+Oqrr5g+fTqnnnoqQRCU+FqzZs2iZcuWNGzYsLCtW7durF69mi+++GKbn8t7771HmzaRf3PXrFnDgAEDeO+993j//ffZb7/96NmzJ2vWrInoN2zYME455RQ+++wzzjvvPN59913OOeccrrjiCr788kseeeQRxowZUxiqABISEnjggQf44osveOqpp3jrrbe49tprt1kfQI8ePahZs+Y2vw4++OBtjp09ezY5OTl07ty5sK1FixbstddezJo1a5vj8vPz6d+/P9dcc812n7/Ab7/9xtixY+nQoUNEyNprr71o2LAh7777btTn2CnBLub3338PgOD333+PdylBEARBdnZ28NJLLwXZ2dnxLkVVgPNFsXLOlJ0NGzYEX375ZbBhw4bCtiZBEBCHryYx1D1gwIDg5JNPDoIgCPLz84MpU6YEqampwdVXX114vGHDhsGmTZuCIAiCvLy8YPTo0cEBBxwQ5OfnFz7Ppk2bgvT09OD1118PgiAI9thjj+Dvf//7Nl8XCF588cUgCILgsssuC44//viI59tW33/9619BnTp1grVr1xYef/XVV4OEhIRgyZIlhTU3bdo0yM3NLexzxhlnBH369NnuZ9G0adMgJSUlqFGjRpCUlBQAQd26dYP58+cHQRAE7777bpCZmRls3LgxYty+++4bPPLII0EQBEH79u2Ds846a7uvU9RHH30UAMGaNWuCIAiCadOmBUCwcuXKIAiC4Mknnwx22223bY5/5JFHglq1agW//vpriceLfn8LXHHFFUGnTp0KH3fq1Ck4/PDDI/rk5OQE9erVC55++unCtr59+xZ+hhs3bgwyMjKCmTNnRow7//zzg759+0a05eXlBStXrgy+/vrrAAhmzJhReGzFihVBenp68OyzzwZBEAQrV64MgGDatGnbfM+vv/56kJCQEMybN6/E49E+syAIgoMPPjh48MEHgyAIgueffz7IzMwMVq9eXazf7NmzAyBYuHDhdp+vwIUXXhh07do1om3dunUBEEyaNKnEMQXv+Z133tnuc+fl5QW1atUKXn755cI2ILjyyisj+p1wwgnBHXfcEdH273//O2jcuPE2n3vChAnB7rvvvt3X/+mnn4L58+dv82t7n9HYsWODlJSUYu1HHnlkcO211xZrL5gzt99+e9ClS5fCnw1NmzYN/vGPfxTrf+211wYZGRkBEPzhD38IVqxYUazP4YcfHgwbNqzE+kr6+V0glmyQVL6xTJKk6qtRFXndV155hZo1a5KTk0N+fj79+vVj2LBhhcdbtmwZsery+eef8+233xa7/mTjxo0sWLCAZcuW8csvv3DCCSeU6vXPPfdcunTpwgEHHED37t056aSTtrm72FdffUWrVq2oUaNGYdtRRx1Ffn4+8+bNK/yX/oMPPjhiBaJx48Z89tlnANxxxx3ccccdhce+/PJL9tprLwCuueYazj33XBYvXsw111zDJZdcUrhSNnfuXNauXcvuu+8eUdOGDRsKTwGaM2cOF1544Tbf6+zZsxk2bBhz585l5cqVhStDixYt4qCDDirV51XUnDlzOPzww6lbt27MY4tq3bp1xOOkpCTOPPNMxo4dS//+/Vm3bh3/+9//GDduHADffvst69evp0uXLhHjsrOzOfzww0t8ja+++oqkpCTatWtX2Lb77rtzwAEH8NVXX5W61jlz5rDnnnuy//77l6r/2rVrGTZsGK+++iqLFy8mNzeXDRs2FK44denShaZNm7LPPvvQvXt3unfvzimnnEJGRgatWrXihBNOoGXLlnTr1o2uXbty+umnU6dOnVLXG82GDRsASEtLi2hfunQpN9xwA9OnT2fZsmXk5eWxfv36wroLbL1SNXfuXGbMmBGxwpSXl8fGjRtZv349GRkZvPnmmwwfPpyvv/6a1atXk5ubG3G8JE2aNCmLt1tqc+bM4YEHHuCTTz6JeoPaa665hvPPP58ffviBm2++mXPOOYdXXnklYlx6ejrr168v15rjGpzeeecd7rnnHmbPns3ixYt58cUXi52nu7Xp06czePBgvvjiC7Kysrjhhht2aglckqQd9XG8Cyil4447jocffpiUlBT22GMPkpIi//dfNKRA+HS51q1bl7hLVf369UlIiO1M/yOOOILvv/+e1157jTfffJMzzzyTzp07R71eZnu2vhYiFAoVhpSLL76YM888s/BY0QvO69WrR/PmzWnevDkTJkygZcuWtGnThoMOOoi1a9fSuHHjEq+7Kdj+Oj09fZs1FZxm2K1bN8aOHUv9+vVZtGgR3bp12+ENGbb3ehA+JSvY6rSykq712vp7DHDWWWfRqVMnli1bxpQpU0hPT6d79+4Ahafwvfrqq8V+oS7vjTyiveetXX311UyZMoV7772X5s2bk56ezumnn174mdeqVYtPPvmE6dOn88Ybb3DTTTcxbNgwPvroI2rXrs2UKVOYOXMmb7zxBg8++CB///vf+eCDD9h7772LvVajRo348MMPI9oKdo5r1Kjkf9LYfffdCYVCrFy5MqJ9wIAB/Prrr9x///00bdqU1NRU2rdvX2yubP29W7t2LTfffHOJGy+kpaWxcOFCTjrpJP7yl79w++23U7duXd577z3OP/98srOztxmcevTosd1T3Zo2bbrN0xEbNWpEdnY2q1atitgqfunSpdv8XGbNmsWyZcsK/1EDwgHwr3/9KyNHjoy4prJevXrUq1eP/fffnwMPPJCsrCzef/992rdvX9jnt99+o379+tusvyzENTitW7eOVq1acd5555Vq143vv/+eE088kYsvvpixY8cydepULrjgAho3bky3bt0qoGJJkqqeGjVqbPP6o5K0atWKl156iQYNGpCZmVlin2bNmjF16lSOO+64Uj1nZmYmffr0oU+fPpx++ul0796d3377rdhKyoEHHsiYMWNYt25d4S+MM2bMICEhgQMOOKBUr1W3bt1SrdBkZWXRp08fhgwZUnix/ZIlS0hKStrmNViHHnooU6dOZeDAgcWOff311/z666/ceeedZGVlAeHra3bGoYceymOPPVbiZwXhIPv5559HtM2ZM6fEi+y31qFDB7Kyshg/fjyvvfYaZ5xxRuG4gw46iNTUVBYtWkSnTp1KVeuBBx5Ibm4uH3zwQeG1S7/++ivz5s2LabXt0EMP5aeffuKbb74p1arTjBkzOPfccwuvzVq7dm2xjUySkpLo3LkznTt3ZujQodSuXZu33nqLU089lVAoxFFHHcVRRx3FTTfdRNOmTXnxxRcZPHhwsddq3749t99+O8uWLaNBgwYATJkyhczMzG2+x5SUFA466CC+/PLLiJXWGTNm8NBDD9GzZ08gvCHLihUror7fI444gnnz5m3z7/Ts2bPJz8/nvvvuK/xHjmeffTbq8z722GOFq2Ml2d6cat26NcnJyUydOpXTTjsNgHnz5rFo0aKIcFNUnz59OPHEEyP+IaZbt27079+/xL9fBQr+gWTTpk2FbQWr4dtaDS0rcQ1OPXr0iOm+DaNHj2bvvffmvvvuA8J/Qd977z3+8Y9/VMng9A2wHvi5Zs14lyJJUqEzzjiDUaNGcfLJJ3PLLbew55578sMPP/DCCy9w7bXXsueeezJs2DAuvvhiGjRoQI8ePVizZg0zZszgsssuK/Z8I0aMoHHjxhx++OEkJCQwYcIEGjVqVOJNTM866yyGDh3KgAEDGDZsGMuXL+eyyy6jf//+ERfkl5UrrriCQw45hI8//pjOnTvTvn17evfuzd13383+++/PL7/8UrghRJs2bRg6dCgnnHAC++67L3/605/Izc1l0qRJXHfddey1116kpKTw4IMPcvHFF/P555/v9H2K+vbtyx133EHv3r0ZPnw4jRs35tNPP2WPPfagffv2HH/88dxzzz08/fTTtG/fnv/85z98/vnnpf4Fsl+/fowePZpvvvmGadOmFbbXqlWLq6++mquuuor8/HyOPvpofv/9d2bMmEFmZiYDBgwo9lz77bcfJ598MhdeeCGPPPIItWrV4vrrr6dJkyacfPLJpX7PnTp14phjjuG0005jxIgRNG/enK+//ppQKFS4Irb1677wwgv06tWLUCjEjTfeGLF5xiuvvMJ3333HMcccQ506dZg0aRL5+fkccMABfPDBB0ydOpWuXbvSoEEDPvjgA5YvX164u+TWunbtykEHHUT//v25++67WbJkCTfccAOXXnrpdlfiunXrxnvvvRdxw9/99tuvcBfG1atXc80115Rqte2mm27ipJNOYq+99uL0008nISGBuXPn8vnnn3PbbbfRvHlzcnJyePDBB+nVq1fhhiDR7Mypervtthvnn38+gwcPpm7dumRmZnLZZZfRvn17/vCHPxT2a9GiBcOHD+fkk0+mbt26NGvWLCI4JScn06hRo8J/JPnggw/46KOPOProo6lTpw4LFizgxhtvZN99940IZO+//37hil15qlLXOM2aNStitw4IT8Tt3XV606ZNEYl09erVQHgZO9574XdOSuLH5GTqdujAuZV4X35VHgVzNt5zV1WHc6bs5OTkEAQB+fn5FXKH+rISbN5TYls1b308CAIyMjKYNm0aQ4YM4dRTT2XNmjU0adKE448/npo1axbuhLV+/Xruv/9+rr76aurVq8dpp50W8ToFn1WNGjW4++67mT9/PomJiRx55JGFW0sX9C/om5aWxmuvvcZVV13FkUceSUZGBqeeeir33XdfRI1bv6eC09WifW+2HteiRQu6dOnCjTfeyKuvvsorr7zCDTfcwMCBA1m+fDmNGjWiY8eO1K9fn/z8fI455hjGjx/P7bffzp133klmZiYdO3YkPz+f3XffnSeeeIIbbriBBx54gCOOOIK7776b3r17F76/rd9v0cclSUpKYvLkyVx99dX07NmT3NxcDjroIB588EHy8/Pp0qULN9xwA9deey0bN25k4MCB9O/fn88//7zY51PSa/Tt25fbb7+dpk2b0r59+4g+N998M/Xq1WP48OF899131K5dm8MPP5whQ4aU+NkHQcDjjz/OlVdeyUknnUR2djYdO3bklVdeITExscT3vy0TJkzgmmuuoW/fvqxbt47mzZtzxx13lPiZ3XvvvVxwwQV06NCBevXqce2117J69erC95yZmckLL7zAsGHD2LhxI/vttx9jx47lwAMP5KuvvuLtt99m5MiRrF69mqZNm3LvvffSrVu3EusLhUJMnDiRSy65hPbt21OjRg3OOecchg0btt33M3DgQNq2bcvKlSvZbbfdAHj00Ue5+OKLOeKII8jKyuK2227j2muvLfa92vqz6tKlCxMnTuS2227jrrvuIjk5mRYtWnDeeeeRn59Py5Ytue+++7jrrrsYMmQIHTt25Pbbb+fcc88t159f9913H6FQiNNOO41NmzbRtWtXRo0aFfF68+bNY+XKlRFzZut6iralpaXxwgsvMHToUNatW1d4ltm4ceNITk4u7PfMM8/Qr18/0tLSSnx/+fn5BEFATk5Osd0ZY/n/YyjY+sTYOAmFQlGvcdp///0ZOHAgQ4YMKWybNGkSJ554IuvXry8xpQ8bNoybb765WPszzzyzzXM8K8oFXbqwIiODuhs28MQbb8S1FknS9iUlJdGoUSOysrIiNlKQpNI499xzOfTQQ0s8BVA77tdff+XII49k2rRpNG3atMQ+2dnZ/PjjjyxZsiTiflcA69evp1+/fvz+++/bPDW5QJVacdoRQ4YMiZigq1evJisri65du0b9cMpbepGLc7t06VKq85G1a8vJyWHKlCnOF5Wac6bsbNy4kR9//JGaNWsW2x2rOgmCgDVr1lCrVq2oO11J4JwprREjRvDKK6/E/ffPyqAs58w333zDqFGjaNmy5Tb7bNy4kfT0dI455phiP78LzkYrjSoVnBo1alS4c0mBpUuXkpmZuc1zQlNTU0s85zQ5OblS/RJR2epR5eZ8UaycMzsvLy+PUChEQkJCzLvKVSUFp7kUvFcpGudM6eyzzz5cfvnl8S6jUijLOdO2bduIm1CXJCEhgVAoVOL/C2P5f2OVmt3t27dn6tSpEW1Tpkwp9wvBJEmSJO3a4hqc1q5dy5w5c5gzZw4Q3m58zpw5hTf+GjJkCOecc05h/4svvpjvvvuOa6+9lq+//pqHHnqIZ599lquuuioe5UuSJEnaRcQ1OH388cccfvjhhVtmDh48mMMPP5ybbroJgMWLF0fcPXnvvffm1VdfZcqUKbRq1Yr77ruPxx57rEpuRS5JqpoqyZ5KkqRSKquf23G9xunYY4/d7hsZM2ZMiWM+/fTTcqxKkqTiCrawzc7OLtW9ViRJlUN2djZAsa3IY1WlNoeQJClekpKSyMjIYPny5SQnJ1fbi+Dz8/PJzs5m48aN1fY9qmw5ZxSripwz+fn5LF++nIyMDJKSdi76GJwkSSqFUChE48aN+f777/nhhx/iXU65CYKADRs2kJ6e7tbSKhXnjGJV0XMmISGBvfbaa6dfy+AkSVIppaSksN9++xWe9lEd5eTk8M4773DMMce4hb1KxTmjWFX0nElJSSmTlS2DkyRJMUhISKjWN8BNTEwkNzeXtLQ0fwlWqThnFKuqOmc8EVWSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYoi7sFp1KhRNGvWjLS0NNq1a8eHH3643f4jR47kgAMOID09naysLK666io2btxYQdVKkiRJ2hXFNTiNHz+ewYMHM3ToUD755BNatWpFt27dWLZsWYn9n3nmGa6//nqGDh3KV199xeOPP8748eP529/+VsGVS5IkSdqVxDU4jRgxggsvvJCBAwdy0EEHMXr0aDIyMnjiiSdK7D9z5kyOOuoo+vXrR7NmzejatSt9+/aNukolSZIkSTsjKV4vnJ2dzezZsxkyZEhhW0JCAp07d2bWrFkljunQoQP/+c9/+PDDD2nbti3fffcdkyZNon///tt8nU2bNrFp06bCx6tXrwYgJyeHnJycMno3OygpCUKhwnqkaArmifNFpeWcUaycM4qVc0axqkxzJpYa4hacVqxYQV5eHg0bNoxob9iwIV9//XWJY/r168eKFSs4+uijCYKA3NxcLr744u2eqjd8+HBuvvnmYu1vvPEGGRkZO/cmdtKGLl1gcw1TpkyJay2qWpwvipVzRrFyzihWzhnFqjLMmfXr15e6b9yC046YPn06d9xxBw899BDt2rXj22+/5YorruDWW2/lxhtvLHHMkCFDGDx4cOHj1atXk5WVRdeuXcnMzKyo0kuUnrTl4+/SpQvJyclxrEZVQU5ODlOmTHG+qNScM4qVc0axcs4oVpVpzhScjVYacQtO9erVIzExkaVLl0a0L126lEaNGpU45sYbb6R///5ccMEFALRs2ZJ169bx5z//mb///e8kJBS/ZCs1NZXU1NRi7cnJyXH/RhVV2epR5eZ8UaycM4qVc0axcs4oVpVhzsTy+nHbHCIlJYXWrVszderUwrb8/HymTp1K+/btSxyzfv36YuEoMTERgCAIyq9YSZIkSbu0uJ6qN3jwYAYMGECbNm1o27YtI0eOZN26dQwcOBCAc845hyZNmjB8+HAAevXqxYgRIzj88MMLT9W78cYb6dWrV2GAkiRJkqSyFtfg1KdPH5YvX85NN93EkiVLOOyww5g8eXLhhhGLFi2KWGG64YYbCIVC3HDDDfz888/Ur1+fXr16cfvtt8frLUiSJEnaBcR9c4hBgwYxaNCgEo9Nnz494nFSUhJDhw5l6NChFVCZJEmSJIXF9Qa4kiRJklQVGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJqnBBvAuIUdyD06hRo2jWrBlpaWm0a9eODz/8cLv9V61axaWXXkrjxo1JTU1l//33Z9KkSRVUrSRJkqRYZQMfACOBfomJXNClCx+GQvEtKkZJ8Xzx8ePHM3jwYEaPHk27du0YOXIk3bp1Y968eTRo0KBY/+zsbLp06UKDBg147rnnaNKkCT/88AO1a9eu+OIlSZIklegXYFaRr9nApoKDCQmQkcH7eXkcHaf6dkRcg9OIESO48MILGThwIACjR4/m1Vdf5YknnuD6668v1v+JJ57gt99+Y+bMmSQnJwPQrFmziixZkiRJUhHZwBwig9KiKGPScnNZ7YpT6WRnZzN79myGDBlS2JaQkEDnzp2ZNWtWiWMmTpxI+/btufTSS/nf//5H/fr16devH9dddx2JiYkljtm0aRObNhXmW1avXg1ATk4OOTk5ZfiOdkBSEmyeMHGvRVVCwTxxvqi0nDOKlXNGsXLO7HoWA++HQnwQCvF+KMQnoRAbo4Sg5kFAuyDgD0FA6+xsFk+ZQvfOncnZvBgSL7HM27gFpxUrVpCXl0fDhg0j2hs2bMjXX39d4pjvvvuOt956i7POOotJkybx7bffcskll5CTk8PQoUNLHDN8+HBuvvnmYu1vvPEGGRkZO/9GdsKGLl1gcw1TpkyJay2qWpwvipVzRrFyzihWzpnqKTcUYuFuu/F1nTrMq1uXr+vWZXmU36FTc3PZb+VKDli5kgN++40DVq5kt+zswuPLgEQqx5xZv359qfuGgiCIy4YWv/zyC02aNGHmzJm0b9++sP3aa6/l7bff5oMPPig2Zv/992fjxo18//33hStMI0aM4J577mHx4sUlvk5JK05ZWVmsWLGCzMzMMn5Xsdk3KYkfQyHqbtjAj0FQePqhtC05OTlMmTKFLl26OF9UKs4Zxco5o1g5Z6qXpYRXkwpWlGaHQmyIspq0b5HVpHb5+bRk+6szlWnOrF69mnr16vH7779HzQZxW3GqV68eiYmJLF26NKJ96dKlNGrUqMQxjRs3Jjk5OeK0vAMPPJAlS5aQnZ1NSkpKsTGpqamkpqYWa09OTo77N6qoylaPKjfni2LlnFGsnDOKlXOm6skB/o/Ia5O+jzImHTgSaA90AP4ANAiFCi8/YRuXz5SkMsyZWF4/bsEpJSWF1q1bM3XqVHr37g1Afn4+U6dOZdCgQSWOOeqoo3jmmWfIz88nISG8k/o333xD48aNSwxNkiRJksKWERmSPgI2RBmzN+GQVPB1KLCrxuO47qo3ePBgBgwYQJs2bWjbti0jR45k3bp1hbvsnXPOOTRp0oThw4cD8Je//IV//vOfXHHFFVx22WXMnz+fO+64g8svvzyeb0OSJEmqVHIpvpr0XZQxaWxZTWpPeDWp5PPAdk1xDU59+vRh+fLl3HTTTSxZsoTDDjuMyZMnF24YsWjRosKVJYCsrCxef/11rrrqKg499FCaNGnCFVdcwXXXXRevtyBJkiTF3XLgfbaEpA+BaNseNCNyNakVu+5qUmnENTgBDBo0aJun5k2fPr1YW/v27Xn//ffLuSpJkiSpcsoFPidyNenbKGNSgTaEr0sqWE1qXI41VkdxD06SJEmStm0FxVeT1kUZsxeRq0mHAe4IsHMMTpIkSVIlkUfx1aT5UcakAq2JDEp7lGONuyqDkyRJkhQnv1J8NWltlDFZFF9NKn7zHZU1g5MkSZJUAfKAL4lcTZoXZUwKxVeTmpRjjdo2g5MkSZJUDlYSuZr0AbAmypgmRIakI3A1qbIwOEmSJEk7KZ/iq0lfRxmTTDgYFQ1KWeVYo3aOwUmSJEmK0SrCK0gz2bKatDrKmD0ovpqUVn4lqowZnCRJkqTtyCe8elR0NenLKGOSgcMpvpoUKr8yVc4MTpIkSVIRvxNeQSp6bdKqKGMasSUgdSC8mpRefiUqDgxOkiRJ2mXlE97ZbuvVpGA7Y5IIbwFedDWpKa4mVXcGJ0mSJO0yVlN8NWlllDENiQxJrYGMcqxRlZPBSZIkSdVSAHxD5GrS52x/NSmR4qtJzXA1SQYnSZIkVRNrgA/ZEpLeB36LMqYBkSGpDa4mqWQGJ0mSJFU5ATCf4qtJ+dsZkwgcSmRQ2gdXk1Q6BidJkiRVemspvpr0a5Qx9YgMSUcCNcqxRlVvBidJkiRVKgGwgC0haSbwGdtfTUqg+GrSvriapLKzQ8EpLy+PMWPGMHXqVJYtW0Z+fuQ0fuutt8qkOEmSJFV/64CPiDztbkWUMbtTfDWpZjnWKO1QcLriiisYM2YMJ554IocccgihkFlekiRJ0QXAd0SGpP8D8rYzJgE4hMigtB+uJqli7VBwGjduHM8++yw9e/Ys63okSZJUjawncjXpfWBZlDF1gT+wJSS1BWqVY41SaexQcEpJSaF58+ZlXYskSZKqsAD4nsjVpLlsfzUpRPHVpP1xNUmVzw4Fp7/+9a/cf//9/POf//Q0PUmSpF3UBuD9UIgXmjfnicREPgCWRhlTh+KrSZnlW6ZUJnYoOL333ntMmzaN1157jYMPPpjk5OSI4y+88EKZFCdJkqTKIQB+IHI1aQ6Qm5QEBx9c4pgQcBCRq0kHEL5mSapqdig41a5dm1NOOaWsa5EkSVIlsQGYTWRQWhJlzG5Eria129wmVQc7FJyefPLJsq5DkiRJcRIAiyi+mpQTZdxBQLv8fDLmzuXCQw6hZXKyq0mqtnbqBrjLly9n3rx5ABxwwAHUr1+/TIqSJElS+dkIfELkDWYXRxmTSfHVpNpATl4ekxYt4qBDDjE0qVrboeC0bt06LrvsMp5++unCm98mJiZyzjnn8OCDD5KRkVGmRUqSJGnH/UjkatInRF9NakE4IHXY/N8D8dok7dp2KDgNHjyYt99+m5dffpmjjjoKCG8Ycfnll/PXv/6Vhx9+uEyLlCRJUulsInI1aRbwc5QxtQivIBVdTapbjjVKVdEOBafnn3+e5557jmOPPbawrWfPnqSnp3PmmWcanCRJkirITxRfTcqOMuYAIne6OwhILMcapepgh4LT+vXradiwYbH2Bg0asH79+p0uSpIkScVlA58SGZR+jDKmJsVXk3Yvxxql6mqHglP79u0ZOnQoTz/9NGlpaQBs2LCBm2++mfbt25dpgZIkSbuqX4gMSbMJn4q3PfsTuZp0MK4mSWVhh4LT/fffT7du3dhzzz1p1aoVAHPnziUtLY3XX3+9TAuUJEnaFWQT3gK8aFBaFGVMDaAtW0LSH4B65VeitEvboeB0yCGHMH/+fMaOHcvXX38NQN++fTnrrLNIT08v0wIlSZKqo8UUX03aGGVMcyJXkw5hJ+8tI6nUdvjvWkZGBhdeeGFZ1iJJklQt5QBz2XLPpFnAD1HGZFB8Nck7ZkrxU+rgNHHiRHr06EFycjITJ07cbt8//vGPO12YJElSVbWUyNWkj4ENUcbsS+RqUktcTZIqk1L/fezduzdLliyhQYMG9O7de5v9QqEQeXl5ZVGbJElSpZcD/B+RQen7KGPSgSPZcoPZPwANyrFGSTuv1MEpPz+/xD9LkiTtSpYRGZI+Ivpq0t5EriYdCiSXY42Syl6ZrQCvWrWK2rVrl9XTSZIkxV0uxVeTvosyJo0tq0kF1yY1KscaJVWMHQpOd911F82aNaNPnz4AnHHGGTz//PM0btyYSZMmFW5RLkmSVJUsB95nS0j6EFgfZUwzIleTWuFqklQd7VBwGj16NGPHjgVgypQpvPnmm0yePJlnn32Wa665hjfeeKNMi5QkSSprucDnRK4mfRtlTCrQhvB1SQWrSY3LsUZJlccOBaclS5aQlZUFwCuvvMKZZ55J165dadasGe3atSvTAiVJksrCCoqvJq2LMmYvIleTDgNSyq9ESZXYDgWnOnXq8OOPP5KVlcXkyZO57bbbAAiCwB31JElS3OVRfDVpfpQxqUBrIoPSHuVYo6SqZYeC06mnnkq/fv3Yb7/9+PXXX+nRowcAn376Kc2bNy/TAiVJkqL5leKrSWujjMmi+GpSavmVKKmK26Hg9I9//INmzZrx448/cvfdd1OzZk0AFi9ezCWXXFKmBUqSJBWVB3xJ5GrSvChjUii+mtSkHGuUVP3sUHBKTk7m6quvLtZ+1VVX7XRBkiRJRa0kcjXpA2BNlDFNiAxJR+BqkqSdU+rgNHHiRHr06EFycjITJ07cbt8//vGPO12YJEna9eRTfDXp6yhjkgkHo6JBKasca5S0ayp1cOrduzdLliyhQYMG9O7de5v9QqGQG0RIkqRSWUV4BWkmW1aTVkcZswfFV5PSyq9ESQJiCE75+fkl/lmSJKk08gmvHhVdTfoyyphk4HCKryaFyq9MSSrRDl3jJEmSFM3vhFeQil6btCrKmEZsCUgdCK8mpZdfiZJUajsUnC6//HKaN2/O5ZdfHtH+z3/+k2+//ZaRI0eWRW2SJKmKyCe8s93Wq0nBdsYkEd4CvOhqUlNcTZJUOe1QcHr++edL3CCiQ4cO3HnnnQYnSZKqudUUX01aGWVMQyJDUmsgoxxrlKSytEPB6ddff2W33XYr1p6ZmcmKFSt2uihJklR5BMA3RK4mfc72V5MSKb6a1AxXkyRVXTsUnJo3b87kyZMZNGhQRPtrr73GPvvsUyaFSZKk+NiQlMRboRAfEQ5J7wO/RRnTgMiQ1AZXkyRVLzsUnAYPHsygQYNYvnw5xx9/PABTp07lvvvu8zQ9SZKqkACYz5aVpJlJSXzRsyf5oW2vDSUChxIZlPbB1SRJ1dsOBafzzjuPTZs2cfvtt3PrrbcC0KxZMx5++GHOOeecMi1QkiSVnbXAh2wJSu8DvxbtUEJgqkdkSDoSqFHOdUpSZbPD25H/5S9/4S9/+QvLly8nPT2dmjVrlmVdkiRpJwXAAoqsJgGfEd4Bb1sSgoCmv/9O11q1OCoxkfbAvriaJEk7HJxyc3OZPn06CxYsoF+/fgD88ssvZGZmGqIkSYqT1cDLwAvAO0C0LZt2J3I16bDcXN55+2169uxJcmJiudYqSVXJDgWnH374ge7du7No0SI2bdpEly5dqFWrFnfddRebNm1i9OjRZV2nJEnahjWEw9KzwGRg0zb6JQCHEBmU9iNyNSmn/MqUpCpth4LTFVdcQZs2bZg7dy677757Yfspp5zChRdeWGbFSZKkkq0BXiEcll6j5LBUh8iQ1BaoVVEFSlI1s0PB6d1332XmzJmkpKREtDdr1oyff/65TAqTJEmR1hIZljaW0KcRcDpwBnAU4R3wJEk7b4eCU35+Pnl5ecXaf/rpJ2rV8t+yJEkqKwVhaQIwiZLDUkO2hKWjMSxJUnlI2JFBXbt2jbhfUygUYu3atQwdOpSePXuWVW2SJO2S1gLjgdOA+kBfwps9FA1NDYFLgOnAz8A/gU4YmiSpvOzQitO9995L9+7dOeigg9i4cSP9+vVj/vz51KtXj//+979lXaMkSdXeOuBVwqfhTQI2lNCnAVtWljpiSJKkirRDwSkrK4u5c+cyfvx45s6dy9q1azn//PM566yzSE9PL+saJUmqlgrC0oTN/91WWDqNcFg6BsOSJMVLzMEpJyeHFi1a8Morr3DWWWdx1llnlUddkiRVS+sIryhNIHztUklhqT6RYWmHb7ooSSozMf8sTk5OZuPGki5NlSRJJVlPOCw9S3hlaX0JfeoDpwJnYliSpMpohzaHuPTSS7nrrrvIzc0t63okSaoW1gPPA30Ih6IzCK8yFQ1N9YCLgDeBX4DRwPEYmiSpMtqhn80fffQRU6dO5Y033qBly5bUqFEj4vgLL7xQJsVJklSVrCd8f6WC0/DWldBnd7achncshiRJqip26Od17dq1Oe2008q6FkmSqpwNbAlLL7PtsFRwGt6xGJYkqSqK6Wd3fn4+99xzD9988w3Z2dkcf/zxDBs2zJ30JEm7lA3AZMLXLG0vLJ3ClrCUXFHFSZLKRUzB6fbbb2fYsGF07tyZ9PR0HnjgAZYvX84TTzxRXvVJklQpbCQyLK0toU9dwitLZwDHYViSpOokpuD09NNP89BDD3HRRRcB8Oabb3LiiSfy2GOPkZCwQ/tMSJJUaRWEpQnAREoOS3XYEpaOx7AkSdVVTMFp0aJF9OzZs/Bx586dCYVC/PLLL+y5555lXpwkSRVtI/A6W8LSmhL61GHLaXiGJUnaNcQUnHJzc0lLS4toS05OJicnp0yLkiSpIm0E3iB8Gt72wlJvtoSllIoqTpJUKcQUnIIg4NxzzyU1NbWwbePGjVx88cURW5K7HbkkqbLbxJaVpf9RcliqTXhl6QzgBAxLkrQriyk4DRgwoFjb2WefXWbFSJJUnjYRXlkqCEurS+izG1vCUmcMS5KksJiC05NPPlledUiSVC42AVMIn4a3vbDUm/BpeIYlSVJJvAefJKnaySYyLP1eQp/dgJPZEpZSS+gjSVIBg5MkqVooCEsTgJcoOSxlEl5ZOgPogmFJklR6BidJUpWVDbzJlrC0qoQ+mYRXls4AumJYkiTtGIOTJKlKyQamEj4N7yVKDku12HIanmFJklQWDE6SpEovhy1h6UW2HZb+yJawlFZCH0mSdpTBSZJUKRWEpQmEw9LKEvrUZMtpeN0wLEmSyo/BSZJUaeQAb7ElLP1WQp+ahFeWCsJSeoVVJ0nalRmcJElxlQNMY8tpeNsKS70In4ZnWJIkxYPBSZJU4QrC0gTgBUoOSzXYsrLUHcOSJCm+DE6SpAqRS2RY+rWEPjUIryydAfTAsCRJqjwMTpKkcpMPzAKeIXwq3ooS+mQQGZYyKqw6SZJKLyHeBQCMGjWKZs2akZaWRrt27fjwww9LNW7cuHGEQiF69+5dvgVKkkotAOYC1wN7A0cDDxEZmjIIX6/0HLAcGAechqFJklR5xX3Fafz48QwePJjRo0fTrl07Ro4cSbdu3Zg3bx4NGjTY5riFCxdy9dVX07FjxwqsVpK0LQuA/27++rKE42nASUAfoCeGJElS1RL3FacRI0Zw4YUXMnDgQA466CBGjx5NRkYGTzzxxDbH5OXlcdZZZ3HzzTezzz77VGC1kqSiFgP3A38AmgM3EhmaEgmffvc0sIzw9U2nY2iSJFU9cV1xys7OZvbs2QwZMqSwLSEhgc6dOzNr1qxtjrvlllto0KAB559/Pu++++52X2PTpk1s2rSp8PHq1asByMnJIScnZyffwU5KSoJQqLAeKZqCeeJ8UWmVx5xZBbwUCjEuIYHpoRD5m3+OFXVUfj5/CgJOzc+nftF6yqwKlRd/zihWzhnFqjLNmVhqiGtwWrFiBXl5eTRs2DCivWHDhnz99dcljnnvvfd4/PHHmTNnTqleY/jw4dx8883F2t944w0yMuL7b54bunSBzTVMmTIlrrWoanG+KFY7O2c2JSTwcaNGvLPnnsxu0IDcxMRifZr9/jvH/PQTR//8Mw02bADgo516VcWTP2cUK+eMYlUZ5sz69etL3Tfu1zjFYs2aNfTv359HH32UevXqlWrMkCFDGDx4cOHj1atXk5WVRdeuXcnMzCyvUkslPWnLx9+lSxeSk5PjWI2qgpycHKZMmeJ8UantzJzJAd7avLL0v1CItSWsLO0TBPTJz6dPfj4HZWTA/vuHv1Rl+XNGsXLOKFaVac4UnI1WGnENTvXq1SMxMZGlS5dGtC9dupRGjRoV679gwQIWLlxIr169Ctvy8/MBSEpKYt68eey7774RY1JTU0lNTS32XMnJyXH/RhVV2epR5eZ8UaxKO2fygZmEN3jY1vbhDYE/Af2AI0MhQomJUMIKlKo2f84oVs4ZxaoyzJlYXj+uwSklJYXWrVszderUwi3F8/PzmTp1KoMGDSrWv0WLFnz22WcRbTfccANr1qzh/vvvJysrqyLKlqRqJQD+j/C9lsYBi0rosxvh7cL7AccS3vRBkqRdSdxP1Rs8eDADBgygTZs2tG3blpEjR7Ju3ToGDhwIwDnnnEOTJk0YPnw4aWlpHHLIIRHja9euDVCsXZK0faXZPrwX4bDUAyi+di9J0q4j7sGpT58+LF++nJtuuoklS5Zw2GGHMXny5MINIxYtWkRCQtx3TZekamEx4VPwngFKutV4ItAV6Av0BmpVWGWSJFVucQ9OAIMGDSrx1DyA6dOnb3fsmDFjyr4gSapG1iYlMSYUYjwwjfB1TFs7mvDK0ukQsX24JEkKqxTBSZJUtjYArwD/SUxkUvfuJW4f3opwWOoDNK3Y8iRJqnIMTpJUTeQAUwmfhvcisBZgq1Od9yEclvoCB1VseZIkVWkGJ0mqwkqzfXjtjRvpn5zM2YmJHAkUvxuTJEmKxuAkSVVMLNuHn5mby7rXX6dXz54ke68lSZJ2mMFJkqqIHdk+PCcImFRhFUqSVH0ZnCSpElsCjMftwyVJijeDkyRVQl8CdwNjgdwSjh9NOCydgduHS5JUEQxOklSJvEc4ML1cwjG3D5ckKX4MTpIUZ/mE77l0F+Ed8oqqDVwEnIPbh0uSFE8GJ0mKk2zC1y7dQ/HNHvYEBgMX4HVLkiRVBgYnSapga4B/Af8Aft7q2EHAtYSvX0qp4LokSdK2GZwkqYIsBR4AHgJWbXXsaOA6oCeQULFlSZKkUjA4SVI5+xa4FxgDbNrq2B8JB6YOFVyTJEmKjcFJksrJbMIbPjxPeAOIAsnA2cDVuOGDJElVhcFJkspQALxJODBN3epYTcI75F1JePMHSZJUdRicJKkM5ALPEb4H06dbHWsAXAH8BahTwXVJkqSyYXCSpJ2wAXiS8DVM3291bF/gGsL3YEqv4LokSVLZMjhJ0g74DRgFPAgs3+pYa8IbPpwKJFZwXZIkqXwYnCQpBosI33/pUWDdVse6Eg5MxwGhCq5LkiSVL4OTJJXC54SvX/ov4euZCiQAZxK+ae3hcahLkiRVDIOTJG1DALxHeIe8V7c6lgacDwwG9qnguiRJUsUzOEnSVvKBiYRXmGZtdawOMAi4DKhfwXVJkqT4MThJ0mabgLHAPcDXWx3LIry6dAHh+zFJkqRdi8FJ0i5vNfAvwps+/LLVsUMIX7/0JyC5guuSJEmVh8FJ0i5rCXA/8DDw+1bHOhLeIa8n7pAnSZIMTpJ2QfMJ37D2KcKn5xXVm/AKU/sKrkmSJFVuBidJu4yPCO+Q9wLhHfMKJAP9gWuAFnGoS5IkVX4GJ0nVWgC8QTgwTdvqWC3gYuBKYI+KLUuSJFUxBidJ1VIu8CzhLcXnbnWsIeGwdDFQu0KrkiRJVZXBSVK1shF4nPA1TAu3OrYf4dPx+hO+ga0kSVJpGZwkVQubCAemO4Cftzp2JOEd8noDiRVbliRJqiYMTpKqtGxgDHAb8ONWx7oRDkzH4pbikiRp5xicJFVJOcC/gVspfkreycBQ4PAKrkmSJFVfBidJVUou8AxwC7Bgq2MnAsOANhVckyRJqv4MTpKqhDxgPHAz8M1Wx7ptbm9X0UVJkqRdhsFJUqUWAK8CfwM+2+rYCYQD01EVXZQkSdrlGJwkVVrvAdcDM7Zq70T4VL1jKrwiSZK0qzI4Sap0/g/4O/DKVu1tCW83fjzukidJkiqWwUlSpfE9cBMwlvApegVaEA5MvTEwSZKk+DA4SYq7pcDtwGjC24wX2JPwNUzn4A8rSZIUX/4uIilufgfuA0YA64q01yV8qt4lQFoc6pIkSdqawUlShdsIPET49Ltfi7RnAIOBq4Hd4lCXJEnSthicJFWYXODfwFDgxyLtycBFwA1AwzjUJUmSFI3BSVK5C4CXCJ9+91WR9hDQj/DW4vtUfFmSJEmlZnCSVK6mE74X0wdbtZ9IeEOIVhVdkCRJ0g4wOEkqF3MIB6bXt2rvANwJdKzogiRJknZCQrwLkFS9LAMuBI4gMjQdAkwE3sPQJEmSqh5XnCSViWzgQcLXK60u0t4UuJXwtUyJcahLkiSpLBicJO2UAHiV8Dbi84u0ZwI3ApcBqXGoS5IkqSwZnCTtsK+Aq4g8JS8EnA/chluLS5Kk6sPgJClmK4GbgX8CeUXaOwL3A4fHoyhJkqRyZHCSVGq5wKOET8H7tUj7XsA9wBmEV5wkSZKqG4OTpFJ5C7gS+KxIWzrhLcevBjLiUJMkSVJFMThJ2q7vgGuAF7Zq70f4fkxZFV6RJElSxTM4SSrRGmA4MALYVKS9NeHrmI6KR1GSJElxYnCSFCEf+A/hU/AWF2lvSDhIDcA7Z0uSpF2PwUlSofeBK4APi7SlEN5y/G+E780kSZK0KzI4SeJnwitM/9mq/WTgXqB5hVckSZJUuRicpF3YBuA+wqfgrS/SfjAwEugch5okSZIqI4OTtAsKgOcJbyP+Q5H2usAtwEX4w0GSJKkofzeSdjFzCN+P6e0ibYnAJcAwwuFJkiRJkQxO0i5iOXAD8BjhnfMKdAH+Qfj0PEmSJJXM4CRVc9nAKOBm4Pci7fsSvkdTLyAUh7okSZKqEoOTVI1NAS4D5hVpqwXcCFwOpMajKEmSpCrI4CRVQ0uAwcB/i7SFgIHA7UCjeBQlSZJUhRmcpGokDxhN+Ga1q4u0dwDuB9rEoyhJkqRqwOAkVROfEN5G/OMibbsD9wADgIR4FCVJklRN+LuUVMWtBq4AjiQyNJ0HfE349Dz/okuSJO0cV5ykKioAniMcmhYXaT8YeBjoGI+iJEmSqin/IVqqgr4DegJnsiU0pQN3Ej5lz9AkSZJUtlxxkqqQXODF5s15NimJDUXaTwIeBJrFpSpJkqTqz+AkVRGfAOcnJTHn4IML2/YkHJhOxpvYSpIklSdP1ZMqufXAtUBbYE4oHI9CQcAVwJdAbwxNkiRJ5c0VJ6kSe5PwFuPfFWnba/VqnsnI4Kgk//pKkiRVFFecpEroV+BcoAtbQlMqcHNeHvdNn07bIIhXaZIkSbskg5NUiQTAOOBA4Kki7ccAc4Eh+fkkG5okSZIqnMFJqiR+AnoBfYHlm9t2A/4FTAMOiFNdkiRJ8honKe4C4AlgMLC6SPuphHfM2yMeRUmSJCmCwUmKo0XAhcAbRdoaA6OAU+JSkSRJkkriqXpSHATAI8AhRIamAcAXGJokSZIqG1ecpAq2ELgAmFqkrQnha5l6xqMgSZIkReWKk1RB8gmfgncIkaHpfMKrTIYmSZKkyssVJ6kCLCAckN4u0pYFPAp0i0tFkiRJioUrTlI5CoCHgEOJDE0XAZ9jaJIkSaoqXHGSyslPwHnAlCJtzYDHgBPiUZAkSZJ2mCtOUhkLgP8QvpapaGj6C/AZhiZJkqSqyBUnqQwtJxyQni/S1oTwDW67xqUiSZIklYVKseI0atQomjVrRlpaGu3atePDDz/cZt9HH32Ujh07UqdOHerUqUPnzp2321+qKBMJrzIVDU1nEV5lMjRJkiRVbXEPTuPHj2fw4MEMHTqUTz75hFatWtGtWzeWLVtWYv/p06fTt29fpk2bxqxZs8jKyqJr1678/PPPFVy5FLaa8LVMJwMFs3Z3YALhU/bqxKkuSZIklZ24B6cRI0Zw4YUXMnDgQA466CBGjx5NRkYGTzzxRIn9x44dyyWXXMJhhx1GixYteOyxx8jPz2fq1Kkl9pfK0zTCO+Y9WaStF+Ed806PS0WSJEkqD3G9xik7O5vZs2czZMiQwraEhAQ6d+7MrFmzSvUc69evJycnh7p165Z4fNOmTWzatKnw8erVqwHIyckhJydnJ6ovA0lJEAoV1qOqYxNwU0IC/0hMLGyrFQTcl5fHgCAgBJTHd7RgnjhfVFrOGcXKOaNYOWcUq8o0Z2KpIa7BacWKFeTl5dGwYcOI9oYNG/L111+X6jmuu+469thjDzp37lzi8eHDh3PzzTcXa3/jjTfIyMiIvegytKFLF9hcw5QpU6L0VmXxY61a3Ne6NQt3262w7eAVK7j8k09osGEDr1VADc4Xxco5o1g5ZxQr54xiVRnmzPr160vdt0rvqnfnnXcybtw4pk+fTlpaWol9hgwZwuDBgwsfr169uvC6qMzMzIoqtUTpSVs+/i5dupCcnBzHahRNADySkMC1CQls3LxSmBIE3Jafz+W77UbCcceVew05OTlMmTLF+aJSc84oVs4Zxco5o1hVpjlTcDZaacQ1ONWrV4/ExESWLl0a0b506VIaNWq03bH33nsvd955J2+++SaHHnroNvulpqaSmpparD05OTnu36iiKls9irSM8AYQrxZpOwh4JhSiVWIiFDllryI4XxQr54xi5ZxRrJwzilVlmDOxvH5cN4dISUmhdevWERs7FGz00L59+22Ou/vuu7n11luZPHkybdq0qYhStQt7DWhJZGgaBHwMtIpLRZIkSapocT9Vb/DgwQwYMIA2bdrQtm1bRo4cybp16xg4cCAA55xzDk2aNGH48OEA3HXXXdx0000888wzNGvWjCVLlgBQs2ZNatasGbf3oepnA3Ad8GCRtgaEd9DrGZeKJEmSFC9xD059+vRh+fLl3HTTTSxZsoTDDjuMyZMnF24YsWjRIhIStiyMPfzww2RnZ3P66ZGbPQ8dOpRhw4ZVZOmqxv4P6Ad8UaStJ/AE0LDEEZIkSarO4h6cAAYNGsSgQYNKPDZ9+vSIxwsXLiz/grTLCgivMF0DZG9uSwPuBS4BQnGqS5IkSfFVKYKTVBn8RngDiP8VaTsUeAY4OC4VSZIkqbKI6+YQUmXxHnAYkaHpSuBDDE2SJEkyOGkXlwfcDhwL/Li5bXfgZeAfQPGN7CVJkrQr8lQ97bKWAP2BN4u0HQOMBfaMS0WSJEmqrFxx0i5pCuF7MBWEphBwEzAVQ5MkSZKKc8VJu5QcYChwJ+Ed9AAaE15lOi5eRUmSJKnSMzhpl7EI6AvMLNLWHXiK8I1tJUmSpG3xVD3tEiYR3jWvIDQlAfcAr2JokiRJUnSuOKlayyN8at7tRdqaAeOAdvEoSJIkSVWSwUnV1lKgH/BWkbaTgTFA7TjUI0mSpKrLU/VULb0LHM6W0JRI+NS8FzE0SZIkKXauOKlaCYD7gOsJn6YH4V3zxgMd41WUJEmSqjyDk6qNVcBA4KUibccBzwCN4lCPJEmSqg9P1VO1MAdoQ2Ro+hvwBoYmSZIk7TxXnFTlPQ5cCmza/LgO8G/gxLhVJEmSpOrG4KQqaxNwOfCvIm1tgAmEtxyXJEmSyoqn6qlK+gU4lsjQ9BfgPQxNkiRJKnuuOKnKmQGcDizZ/DgVeAQYELeKJEmSVN254qQqIwAeJrxTXkFo2otwkDI0SZIkqTy54qQqYSPhDSCeKNJ2HOH7M9WPS0WSJEnalbjipErvR+AYIkPTYMJbjRuaJEmSVBFccVKl9g5wBrBs8+N04DGgX9wqkiRJ0q7IFSdVSgHwIHACW0JTM2AmhiZJkiRVPIOTKp0c4GLC92jK3dzWBfgYOCxONUmSJGnXZnBSpbIS6E7k/ZmuA14Ddo9LRZIkSZLXOKkS+RY4Efhm8+MUwhtCnBW3iiRJkqQwg5MqhbeBU4HfNj+uD7wEdIhXQZIkSVIRnqqnuHuS8DVMBaHpYOBDDE2SJEmqPAxOipt84HrgPMIbQkD4+qaZhHfQkyRJkioLT9VTXKwD+gMvFmm7DBiBk1KSJEmVj7+jqsL9DPwR+GTz40TgfuDSuFUkSZIkbZ/BSRXqE6AX8Mvmx5nAs0C3uFUkSZIkRec1TqowLwEd2RKamhG+nsnQJEmSpMrO4KRyFwB3E95ufP3mtg6Ed847OF5FSZIkSTEwOKlcZQPnA9cRDlAAZwNTCd+rSZIkSaoKDE4qN78CXQnfp6nArcDTQFpcKpIkSZJ2jJtDqFzMA04Cvt38OA14CjgzbhVJkiRJO87gpDL3FnAasGrz44bARKBtvAqSJEmSdpKn6qlMPUZ4l7xVmx8fSngTCEOTJEmSqjKDk8pEHnA1cCGQu7ntJOA9YK94FSVJkiSVEU/V007bAPQjfJ+mAlcB9wCJ8ShIkiRJKmMGJ+2U34A/AjM2P04ERgEXxa0iSZIkqewZnLTDfgS6A19uflwTeJ7wFuSSJElSdWJw0g75nHBo+nnz44bAJOCIuFUkSZIklR83h1DM3gE6siU0NQdmYmiSJElS9WVwUkxeIHwq3qrNj48kfH3TPvEqSJIkSaoABieV2kPA6cCmzY+7E77ZbYO4VSRJkiRVDK9x0jYFhG9o+waQDUwscmwA8CiQHIe6JEmSpIpmcFKJAuAa4L4Sjg0BbgdCFVqRJEmSFD8GJxUTAIOBkVu1h4D7gcsquiBJkiQpzgxOipBPOBg9VMKx8cAZFVuOJEmSVCkYnFQoH7iI8HVNEF5hOgGoDVwJHBWXqiRJkqT4MzgJgDzgPODpzY8TgKeAs+NWkSRJklR5GJxEDnAOMG7z40RgLNAnbhVJkiRJlYvBaReVA2wEUoG+hG9sC+HtxccDp8SpLkmSJKkyMjjtgr4EegI/AfWApZvbU4DngZPiVJckSZJUWRmcdjE/AAcXeVwQmtKA/wFdK7wiSZIkqfIzOO1CllNyMMoAXgaOr9hyJEmSpCrD4LSLWA30AL4p4dhkoGPFliNJkiRVKQanXcBGoDcwe/PjPYC9CG9B/gDwh/iUJUmSJFUZBqdqLhfoB0zb/LguMAU4KG4VSZIkSVVPQrwLUPkJgIuBFzc/rgFMwtAkSZIkxcrgVI39DXh885+TCd+rqV38ypEkSZKqLINTNXUfcOfmP4eAf+NW45IkSdKOMjhVQ2OAq4s8HgX0iU8pkiRJUrVgcKpmJgIXFHl8C/CXONUiSZIkVRcGp2rkPeBMwtuMA1wO3BC/ciRJkqRqw+BUTcwD/ghs2vz4LOAfhK9vkiRJkrRzDE7VwFKgB7By8+OuwJP4zZUkSZLKir9bV3HrgJOA7zc/Pgx4jvD245IkSZLKhsGpCssF/gR8vPlxFvAqUCtuFUmSJEnVk8GpigoIb/7wyubHuwGTgD3iVpEkSZJUfRmcqqh7gIc3/zkZeAE4JH7lSJIkSdWawakKGgdcV+TxE8DxcapFkiRJ2hUYnKqYd4ABRR7fBpwdp1okSZKkXYXBqQr5CjgZyN78+ALgb/ErR5IkSdplGJyqgFxgNHAQsGpzW3fC1zh5g1tJkiSp/CXFuwBtX0B4lWlSkbbDgWfxmydJkiRVFFecKrmRRIamJLxXkyRJklTRDE6V2HvAtVu1zQEaV3wpkiRJ0i7N4FRJLQXOJHx9E8DRwG/AwXGrSJIkSdp1eZlMJZQL9AUWb358HPAGfrMkSZKkeHHFqRK6EZi2+c+Ngf9iaJIkSZLiyeBUyUwE7tz850TCu+c1jF85kiRJkjA4VSoLgHOKPL6H8LVNkiRJkuLL4FRJbABOB37f/Ph04Mq4VSNJkiSpKINTJXFlYiJzNv95f+BxIBS/ciRJkiQVUSmC06hRo2jWrBlpaWm0a9eODz/8cLv9J0yYQIsWLUhLS6Nly5ZMmjRpu/0ru5VpaTyZEP5WZADPA5lxrUiSJElSUXEPTuPHj2fw4MEMHTqUTz75hFatWtGtWzeWLVtWYv+ZM2fSt29fzj//fD799FN69+5N7969+fzzzyu48rIThLasLT0CHBK/UiRJkiSVIO7BacSIEVx44YUMHDiQgw46iNGjR5ORkcETTzxRYv/777+f7t27c80113DggQdy6623csQRR/DPf/6zgisve38Bzo53EZIkSZKKievtgbKzs5k9ezZDhgwpbEtISKBz587MmjWrxDGzZs1i8ODBEW3dunXjpZdeKrH/pk2b2LRpU+Hj1atXA5CTk0NOTs5OvoOdlJQEm1ebWuflcXd+PnGuSJVcwZyN+9xVleGcUaycM4qVc0axqkxzJpYa4hqcVqxYQV5eHg0bRt6pqGHDhnz99dcljlmyZEmJ/ZcsWVJi/+HDh3PzzTcXa3/jjTfIyMjYwcrLRtoxx0CdOtTKzuai6dOZumFDXOtR1TFlypR4l6AqxjmjWDlnFCvnjGJVGebM+vXrS903rsGpIgwZMiRihWr16tVkZWXRtWtXMjPjuwVDM2BMTg57v/ceZx19NMnJyXGtR5VfTk4OU6ZMoUuXLs4XlYpzRrFyzihWzhnFqjLNmYKz0UojrsGpXr16JCYmsnTp0oj2pUuX0qhRoxLHNGrUKKb+qamppKamFmtPTk6O+zeqFXB3Tg6T1qypFPWo6nC+KFbOGcXKOaNYOWcUq8owZ2J5/bhuDpGSkkLr1q2ZOnVqYVt+fj5Tp06lffv2JY5p3759RH8IL/Ntq78kSZIk7ay4n6o3ePBgBgwYQJs2bWjbti0jR45k3bp1DBw4EIBzzjmHJk2aMHz4cACuuOIKOnXqxH333ceJJ57IuHHj+Pjjj/nXv/4Vz7chSZIkqRqLe3Dq06cPy5cv56abbmLJkiUcdthhTJ48uXADiEWLFpGQsGVhrEOHDjzzzDPccMMN/O1vf2O//fbjpZde4pBDvPuRJEmSpPIR9+AEMGjQIAYNGlTisenTpxdrO+OMMzjjjDPKuSpJkiRJCov7DXAlSZIkqbIzOEmSJElSFAYnSZIkSYrC4CRJkiRJURicJEmSJCkKg5MkSZIkRWFwkiRJkqQoDE6SJEmSFIXBSZIkSZKiMDhJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSokiKdwEVLQgCAFavXh3nSsJycnJYv349q1evJjk5Od7lqJJzvihWzhnFyjmjWDlnFKvKNGcKMkFBRtieXS44rVmzBoCsrKw4VyJJkiSpMlizZg277bbbdvuEgtLEq2okPz+fX375hVq1ahEKheJdDqtXryYrK4sff/yRzMzMeJejSs75olg5ZxQr54xi5ZxRrCrTnAmCgDVr1rDHHnuQkLD9q5h2uRWnhIQE9txzz3iXUUxmZmbcJ46qDueLYuWcUaycM4qVc0axqixzJtpKUwE3h5AkSZKkKAxOkiRJkhSFwSnOUlNTGTp0KKmpqfEuRVWA80Wxcs4oVs4Zxco5o1hV1Tmzy20OIUmSJEmxcsVJkiRJkqIwOEmSJElSFAYnSZIkSYrC4CRJkiRJURicytmoUaNo1qwZaWlptGvXjg8//HC7/SdMmECLFi1IS0ujZcuWTJo0qYIqVWURy5x59NFH6dixI3Xq1KFOnTp07tw56hxT9RPrz5kC48aNIxQK0bt37/ItUJVOrHNm1apVXHrppTRu3JjU1FT2339///+0i4l1zowcOZIDDjiA9PR0srKyuOqqq9i4cWMFVat4e+edd+jVqxd77LEHoVCIl156KeqY6dOnc8QRR5Camkrz5s0ZM2ZMudcZK4NTORo/fjyDBw9m6NChfPLJJ7Rq1Ypu3bqxbNmyEvvPnDmTvn37cv755/Ppp5/Su3dvevfuzeeff17BlSteYp0z06dPp2/fvkybNo1Zs2aRlZVF165d+fnnnyu4csVLrHOmwMKFC7n66qvp2LFjBVWqyiLWOZOdnU2XLl1YuHAhzz33HPPmzePRRx+lSZMmFVy54iXWOfPMM89w/fXXM3ToUL766isef/xxxo8fz9/+9rcKrlzxsm7dOlq1asWoUaNK1f/777/nxBNP5LjjjmPOnDlceeWVXHDBBbz++uvlXGmMApWbtm3bBpdeemnh47y8vGCPPfYIhg8fXmL/M888MzjxxBMj2tq1axdcdNFF5VqnKo9Y58zWcnNzg1q1agVPPfVUeZWoSmZH5kxubm7QoUOH4LHHHgsGDBgQnHzyyRVQqSqLWOfMww8/HOyzzz5BdnZ2RZWoSibWOXPppZcGxx9/fETb4MGDg6OOOqpc61TlBAQvvvjidvtce+21wcEHHxzR1qdPn6Bbt27lWFnsXHEqJ9nZ2cyePZvOnTsXtiUkJNC5c2dmzZpV4phZs2ZF9Afo1q3bNvuretmRObO19evXk5OTQ926dcurTFUiOzpnbrnlFho0aMD5559fEWWqEtmROTNx4kTat2/PpZdeSsOGDTnkkEO44447yMvLq6iyFUc7Mmc6dOjA7NmzC0/n++6775g0aRI9e/askJpV9VSV34GT4l1AdbVixQry8vJo2LBhRHvDhg35+uuvSxyzZMmSEvsvWbKk3OpU5bEjc2Zr1113HXvssUexHz6qnnZkzrz33ns8/vjjzJkzpwIqVGWzI3Pmu+++46233uKss85i0qRJfPvtt1xyySXk5OQwdOjQiihbcbQjc6Zfv36sWLGCo48+miAIyM3N5eKLL/ZUPW3Ttn4HXr16NRs2bCA9PT1OlUVyxUmqJu68807GjRvHiy++SFpaWrzLUSW0Zs0a+vfvz6OPPkq9evXiXY6qiPz8fBo0aMC//vUvWrduTZ8+ffj73//O6NGj412aKqnp06dzxx138NBDD/HJJ5/wwgsv8Oqrr3LrrbfGuzRpp7jiVE7q1atHYmIiS5cujWhfunQpjRo1KnFMo0aNYuqv6mVH5kyBe++9lzvvvJM333yTQw89tDzLVCUS65xZsGABCxcupFevXoVt+fn5ACQlJTFv3jz23Xff8i1acbUjP2caN25McnIyiYmJhW0HHnggS5YsITs7m5SUlHKtWfG1I3PmxhtvpH///lxwwQUAtGzZknXr1vHnP/+Zv//97yQk+O/2irSt34EzMzMrzWoTuOJUblJSUmjdujVTp04tbMvPz2fq1Km0b9++xDHt27eP6A8wZcqUbfZX9bIjcwbg7rvv5tZbb2Xy5Mm0adOmIkpVJRHrnGnRogWfffYZc+bMKfz64x//WLiLUVZWVkWWrzjYkZ8zRx11FN9++21hyAb45ptvaNy4saFpF7Ajc2b9+vXFwlFB8A6CoPyKVZVVZX4HjvfuFNXZuHHjgtTU1GDMmDHBl19+Gfz5z38OateuHSxZsiQIgiDo379/cP311xf2nzFjRpCUlBTce++9wVdffRUMHTo0SE5ODj777LN4vQVVsFjnzJ133hmkpKQEzz33XLB48eLCrzVr1sTrLaiCxTpntuauerueWOfMokWLglq1agWDBg0K5s2bF7zyyitBgwYNgttuuy1eb0EVLNY5M3To0KBWrVrBf//73+C7774L3njjjWDfffcNzjzzzHi9BVWwNWvWBJ9++mnw6aefBkAwYsSI4NNPPw1++OGHIAiC4Prrrw/69+9f2P+7774LMjIygmuuuSb46quvglGjRgWJiYnB5MmT4/UWSmRwKmcPPvhgsNdeewUpKSlB27Ztg/fff7/wWKdOnYIBAwZE9H/22WeD/fffP0hJSQkOPvjg4NVXX63gihVvscyZpk2bBkCxr6FDh1Z84YqbWH/OFGVw2jXFOmdmzpwZtGvXLkhNTQ322Wef4Pbbbw9yc3MruGrFUyxzJicnJxg2bFiw7777BmlpaUFWVlZwySWXBCtXrqz4whUX06ZNK/H3k4J5MmDAgKBTp07Fxhx22GFBSkpKsM8++wRPPvlkhdcdTSgIXDOVJEmSpO3xGidJkiRJisLgJEmSJElRGJwkSZIkKQqDkyRJkiRFYXCSJEmSpCgMTpIkSZIUhcFJkiRJkqIwOEmSJElSFAYnSZJiEAqFeOmllwBYuHAhoVCIOXPmxLUmSVL5MzhJkqqMc889l1AoRCgUIjk5mb333ptrr72WjRs3xrs0SVI1lxTvAiRJikX37t158sknycnJYfbs2QwYMIBQKMRdd90V79IkSdWYK06SpColNTWVRo0akZWVRe/evencuTNTpkwBID8/n+HDh7P33nuTnp5Oq1ateO655yLGf/HFF5x00klkZmZSq1YtOnbsyIIFCwD46KOP6NKlC/Xq1WO33XajU6dOfPLJJxX+HiVJlY/BSZJUZX3++efMnDmTlJQUAIYPH87TTz/N6NGj+eKLL7jqqqs4++yzefvttwH4+eefOeaYY0hNTeWtt95i9uzZnHfeeeTm5gKwZs0aBgwYwHvvvcf777/PfvvtR8+ePVmzZk3c3qMkqXLwVD1JUpXyyiuvULNmTXJzc9m0aRMJCQn885//ZNOmTdxxxx28+eabtG/fHoB99tmH9957j0ceeYROnToxatQodtttN8aNG0dycjIA+++/f+FzH3/88RGv9a9//YvatWvz9ttvc9JJJ1Xcm5QkVToGJ0lSlXLcccfx8MMPs27dOv7xj3+QlJTEaaedxhdffMH69evp0qVLRP/s7GwOP/xwAObMmUPHjh0LQ9PWli5dyg033MD06dNZtmwZeXl5rF+/nkWLFpX7+5IkVW4GJ0lSlVKjRg2aN28OwBNPPEGrVq14/PHHOeSQQwB49dVXadKkScSY1NRUANLT07f73AMGDODXX3/l/vvvp2nTpqSmptK+fXuys7PL4Z1IkqoSg5MkqcpKSEjgb3/7G4MHD+abb74hNTWVRYsW0alTpxL7H3rooTz11FPk5OSUuOo0Y8YMHnroIXr27AnAjz/+yIoVK8r1PUiSqgY3h5AkVWlnnHEGiYmJPPLII1x99dVcddVVPPXUUyxYsIBPPvmEBx98kKeeegqAQYMGsXr1av70pz/x8ccfM3/+fP79738zb948APbbbz/+/e9/89VXX/HBBx9w1llnRV2lkiTtGlxxkiRVaUlJSQwaNIi7776b77//nvr16zN8+HC+++47ateuzRFHHMHf/vY3AHbffXfeeustrrnmGjp16kRiYiKHHXYYRx11FACPP/44f/7znzniiCPIysrijjvu4Oqrr47n25MkVRKhIAiCeBchSZIkSZWZp+pJkiRJUhQGJ0mSJEmKwuAkSZIkSVEYnCRJkiQpCoOTJEmSJEVhcJIkSZKkKAxOkiRJkhSFwUmSJEmSojA4SZIkSVIUBidJkiRJisLgJEmSJElR/D+QyLj71XFuuQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 43.Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "y_bin = label_binarize(y, classes=np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = rf_clf.predict_proba(X_test)\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(y_bin.shape[1]):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(label_binarize(y_test, classes=np.unique(y_test))[:, i], y_pred_proba[:, i])\n",
    "    average_precision[i] = average_precision_score(label_binarize(y_test, classes=np.unique(y_test))[:, i], y_pred_proba[:, i])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue']\n",
    "\n",
    "for i, color in zip(range(y_bin.shape[1]), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label=f'Precision-Recall curve of class {i} (area = {average_precision[i]:0.2f})')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\gaura\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y=load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "estimator=[\n",
    "    (\"Random Foress\",RandomForestClassifier()),\n",
    "    \n",
    "    (\"Logistic Regression\",LogisticRegression())\n",
    "]\n",
    "stacking_clf=StackingClassifier(estimators=estimator,final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train,y_train)\n",
    "y_pred=stacking_clf.predict(X_test)\n",
    "\n",
    "acc=accuracy_score(y_test,y_pred)\n",
    "print(f\"Stacking Classifier Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap: True, MSE: 19.29, R2 Score: 0.99\n",
      "Bootstrap: False, MSE: 28.51, R2 Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# 45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X, y = make_regression(n_samples=1000, n_features=4, n_informative=2, noise=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "bootstrap_options = [True, False]\n",
    "results = []\n",
    "\n",
    "for bootstrap in bootstrap_options:\n",
    "    bagging_reg = BaggingRegressor(bootstrap=bootstrap, n_estimators=10, random_state=42)\n",
    "    bagging_reg.fit(X_train, y_train)\n",
    "    y_pred = bagging_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results.append((bootstrap, mse, r2))\n",
    "\n",
    "for bootstrap, mse, r2 in results:\n",
    "    print(f\"Bootstrap: {bootstrap}, MSE: {mse:.2f}, R2 Score: {r2:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
